{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"about.html","text":"Introducing Predator # Distribute open source performance testing platform for APIs. Requests per second, request latency, and overall system performance and reliability are some fundamental concepts that need to be taken into account when designing a high capacity API. With CI/CD becoming a common deployment methodology, deployments to production are a constant occurrence. So how can you ensure that the core capabilities you initially designed your system to have remain intact? The answer is: use Predator! Predator is a performance platform that can be configured to automatically load your API with scheduled tests that provide in-depth performance metrics. While sipping on your (morning) coffee, you can simply check the test summaries to make sure your system is performing as expected. By comparing test results to results from previous test runs, you can easily identify system-wide regressions. Predator\u2019s integration with custom dashboards, test metrics, and service logs make pinpointing performance bugs painless,especially when you check up on the system on a daily basis. Main Features # Distributed load : Predator supports unlimited number of load generators that produce load concurrently. Real time reports : Predator aggregate all concurrent runs into one beautiful report in real time (latency, rps, status codes and more) Built for the cloud : Predator is built to take advantage of Kubernetes and DC/OS. it's integrated with those platform and is able to manage the load generators life cycles by it self. One click installation : Installed via one click in Kubernetes, DC/OS, or any other machine that has Docker. Supports 5 Different databases : Predator can adjust it self to persist data in Cassandra, Postgres, MySQL, MSSQL and SQLITE out of the box. Scheduled jobs : Run any tests in recurring mode by cron expression. 3rd partry metrics : Predator integrated with Prometheus and Influx, just configure it via the config endpoint or the ui. Rich UI : Predator offers rich UI side by side powerful REST API. Based on artilliery.io : Predator uses artillery as its load engine to fire the requests. The schema of creating tests via api is based on artillery schema. UI # Under the hood # Feature Comparison #","title":"About"},{"location":"about.html#introducing-predator","text":"Distribute open source performance testing platform for APIs. Requests per second, request latency, and overall system performance and reliability are some fundamental concepts that need to be taken into account when designing a high capacity API. With CI/CD becoming a common deployment methodology, deployments to production are a constant occurrence. So how can you ensure that the core capabilities you initially designed your system to have remain intact? The answer is: use Predator! Predator is a performance platform that can be configured to automatically load your API with scheduled tests that provide in-depth performance metrics. While sipping on your (morning) coffee, you can simply check the test summaries to make sure your system is performing as expected. By comparing test results to results from previous test runs, you can easily identify system-wide regressions. Predator\u2019s integration with custom dashboards, test metrics, and service logs make pinpointing performance bugs painless,especially when you check up on the system on a daily basis.","title":"Introducing Predator"},{"location":"about.html#main-features","text":"Distributed load : Predator supports unlimited number of load generators that produce load concurrently. Real time reports : Predator aggregate all concurrent runs into one beautiful report in real time (latency, rps, status codes and more) Built for the cloud : Predator is built to take advantage of Kubernetes and DC/OS. it's integrated with those platform and is able to manage the load generators life cycles by it self. One click installation : Installed via one click in Kubernetes, DC/OS, or any other machine that has Docker. Supports 5 Different databases : Predator can adjust it self to persist data in Cassandra, Postgres, MySQL, MSSQL and SQLITE out of the box. Scheduled jobs : Run any tests in recurring mode by cron expression. 3rd partry metrics : Predator integrated with Prometheus and Influx, just configure it via the config endpoint or the ui. Rich UI : Predator offers rich UI side by side powerful REST API. Based on artilliery.io : Predator uses artillery as its load engine to fire the requests. The schema of creating tests via api is based on artillery schema.","title":"Main Features"},{"location":"about.html#ui","text":"","title":"UI"},{"location":"about.html#under-the-hood","text":"","title":"Under the hood"},{"location":"about.html#feature-comparison","text":"","title":"Feature Comparison"},{"location":"apireference.html","text":"The Predator API gives you advanced control over the tests you create and allows you utilize functionalities not available through the Predator UI. For instance, using the API you can create DSL definitions and reuse them in your requests. Go ahead and dig into our Predator API documentation.","title":"API Reference"},{"location":"configuration.html","text":"Configuration # When running Predator, it is possible to retrieve and update some of the service's configuration during runtime with the /config endpoint, please check the API reference for more details. Below are variables Predator can be configured with. General # Environment Variable Configuration key Description Configurable from UI/API Default value INTERNAL_ADDRESS internal_address The local ip address of your machine \u2713 RUNNER_DOCKER_IMAGE runner_docker_image The predator-runner docker image that will run the test \u2713 zooz/predator-runner:latest RUNNER_CPU runner_cpu Number of CPU use by the each runner \u2713 1 RUNNER_MEMORY runner_memory Max memory to use by each runner \u2713 256 DEFAULT_EMAIL_ADDRESS default_email_address Default email to send final report to, address can be configured \u2713 DEFAULT_WEBHOOK_URL default_webhook_url Default webhook url to send live report statistics to \u2713 Database # Environment Variable Description Configurable from UI/API Default value DATABASE_TYPE Database to integrate Predator with [Cassandra, Postgres, MySQL, MSSQL, SQLITE] x SQLITE DATABASE_NAME Database/Keyspace name x DATABASE_ADDRESS Database address x DATABASE_USERNAME Database username x DATABASE_PASSWORD Database password x Additional parameters for the following chosen databases: Cassandra # Environment Variable Configurable from UI/API Default value CASSANDRA_REPLICATION_FACTOR x 1 CASSANDRA_CONSISTENCY x localQuorum CASSANDRA_KEY_SPACE_STRATEGY x SimpleStrategy CASSANDRA_LOCAL_DATA_CENTER x SQLITE # Environment Variable Description Configurable from UI/API Default value SQLITE_STORATE SQLITE file name x predator Deployment # Environment Variable Description Configurable from UI/API Default value JOB_PLATFORM Type of platform using to run predator (METRONOME,KUBERNETES,DOCKER) x DOCKER Kubernetes # Environment Variable Description Configurable from UI/API Default value KUBERNETES_URL Kubernetes API Url x KUBERNETES_TOKEN Kubernetes API Token x KUBERNETES_NAMESPACE Kubernetes Namespace x Metronome # Environment Variable Description Configurable from UI/API Default value METRONOME_URL Metronome API Url x METRONOME_TOKEN Metronome API Token x Docker # Environment Variable Description Configurable from UI/API Default value DOCKER_HOST Docker engine url (host and port number of docker engine) x DOCKER_CERT_PATH Path to CA certificate directory x Metrics # PROCESS.ENV Variable Configuration key Description Configurable from UI/API Default value METRICS_PLUGIN_NAME metrics_plugin_name Metrics integration to use [prometheus,influx] \u2713 Prometheus # Environment Variable Configuration key Description Configurable from UI/API Default value prometheus_metrics.push_gateway_url Url of push gateway \u2713 prometheus_metrics.buckets_sizes Bucket sizes to configure prometheus \u2713 Influx # Environment Variable Configuration key Description Configurable from UI/API Default value influx_metrics.host Influx db host \u2713 influx_metrics.username Influx db username \u2713 influx_metrics.password Influx db password \u2713 influx_metrics.database Influx db name \u2713 SMTP Server # Environment Variable Configuration key Description Configurable from UI/API Default value SMTP_FROM smtp_server.from the 'from' email address that will be used to send emails \u2713 SMTP_HOST smtp_server.host SMTP host \u2713 SMTP_PORT smtp_server.port SMTP port number \u2713 SMTP_USERNAME smtp_server.username SMTP username \u2713 SMTP_PASSWORD smtp_server.password SMTP password \u2713 SMTP_TIMEOUT smtp_server.timeout timeout to SMTP server in milliseconds \u2713","title":"Configuration"},{"location":"configuration.html#configuration","text":"When running Predator, it is possible to retrieve and update some of the service's configuration during runtime with the /config endpoint, please check the API reference for more details. Below are variables Predator can be configured with.","title":"Configuration"},{"location":"configuration.html#general","text":"Environment Variable Configuration key Description Configurable from UI/API Default value INTERNAL_ADDRESS internal_address The local ip address of your machine \u2713 RUNNER_DOCKER_IMAGE runner_docker_image The predator-runner docker image that will run the test \u2713 zooz/predator-runner:latest RUNNER_CPU runner_cpu Number of CPU use by the each runner \u2713 1 RUNNER_MEMORY runner_memory Max memory to use by each runner \u2713 256 DEFAULT_EMAIL_ADDRESS default_email_address Default email to send final report to, address can be configured \u2713 DEFAULT_WEBHOOK_URL default_webhook_url Default webhook url to send live report statistics to \u2713","title":"General"},{"location":"configuration.html#database","text":"Environment Variable Description Configurable from UI/API Default value DATABASE_TYPE Database to integrate Predator with [Cassandra, Postgres, MySQL, MSSQL, SQLITE] x SQLITE DATABASE_NAME Database/Keyspace name x DATABASE_ADDRESS Database address x DATABASE_USERNAME Database username x DATABASE_PASSWORD Database password x Additional parameters for the following chosen databases:","title":"Database"},{"location":"configuration.html#cassandra","text":"Environment Variable Configurable from UI/API Default value CASSANDRA_REPLICATION_FACTOR x 1 CASSANDRA_CONSISTENCY x localQuorum CASSANDRA_KEY_SPACE_STRATEGY x SimpleStrategy CASSANDRA_LOCAL_DATA_CENTER x","title":"Cassandra"},{"location":"configuration.html#sqlite","text":"Environment Variable Description Configurable from UI/API Default value SQLITE_STORATE SQLITE file name x predator","title":"SQLITE"},{"location":"configuration.html#deployment","text":"Environment Variable Description Configurable from UI/API Default value JOB_PLATFORM Type of platform using to run predator (METRONOME,KUBERNETES,DOCKER) x DOCKER","title":"Deployment"},{"location":"configuration.html#kubernetes","text":"Environment Variable Description Configurable from UI/API Default value KUBERNETES_URL Kubernetes API Url x KUBERNETES_TOKEN Kubernetes API Token x KUBERNETES_NAMESPACE Kubernetes Namespace x","title":"Kubernetes"},{"location":"configuration.html#metronome","text":"Environment Variable Description Configurable from UI/API Default value METRONOME_URL Metronome API Url x METRONOME_TOKEN Metronome API Token x","title":"Metronome"},{"location":"configuration.html#docker","text":"Environment Variable Description Configurable from UI/API Default value DOCKER_HOST Docker engine url (host and port number of docker engine) x DOCKER_CERT_PATH Path to CA certificate directory x","title":"Docker"},{"location":"configuration.html#metrics","text":"PROCESS.ENV Variable Configuration key Description Configurable from UI/API Default value METRICS_PLUGIN_NAME metrics_plugin_name Metrics integration to use [prometheus,influx] \u2713","title":"Metrics"},{"location":"configuration.html#prometheus","text":"Environment Variable Configuration key Description Configurable from UI/API Default value prometheus_metrics.push_gateway_url Url of push gateway \u2713 prometheus_metrics.buckets_sizes Bucket sizes to configure prometheus \u2713","title":"Prometheus"},{"location":"configuration.html#influx","text":"Environment Variable Configuration key Description Configurable from UI/API Default value influx_metrics.host Influx db host \u2713 influx_metrics.username Influx db username \u2713 influx_metrics.password Influx db password \u2713 influx_metrics.database Influx db name \u2713","title":"Influx"},{"location":"configuration.html#smtp-server","text":"Environment Variable Configuration key Description Configurable from UI/API Default value SMTP_FROM smtp_server.from the 'from' email address that will be used to send emails \u2713 SMTP_HOST smtp_server.host SMTP host \u2713 SMTP_PORT smtp_server.port SMTP port number \u2713 SMTP_USERNAME smtp_server.username SMTP username \u2713 SMTP_PASSWORD smtp_server.password SMTP password \u2713 SMTP_TIMEOUT smtp_server.timeout timeout to SMTP server in milliseconds \u2713","title":"SMTP Server"},{"location":"contributing.html","text":"Guidelines # First off, thanks for taking the time to contribute! The following is a set of guidelines for contributing to Predator and its packages, which are hosted in the Zooz Predator project on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request. Submitting a Bug Report # Bugs are tracked as GitHub issues . Provide the following information when submitting a bug: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. Provide specific examples to demonstrate the steps. Include links to files or GitHub projects, or copy pasteable snippets, which you use in those examples. If you're providing snippets in the issue, use Markdown code blocks. Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. Contributing Code Changes # As an open-source project, we welcome and encourage the community to submit patches directly to the project. In our collaborative open source environment, standards for submitting changes help reduce the chaos that can result from an active development community. So here are some high-level steps we suggest you follow when contributing code changes: Fork the project clone locally. Create an upstream remote and sync your local copy before you branch. Branch for each separate piece of work. Do the work, write good commit messages. Commit messages must adhere to commitlint standards. Push to your origin repository. Create a new PR in GitHub. Respond to any code review feedback. Contributing Documentation Changes # Documentation is mega-important. Predator cannot truly succeed without great documentation. It\u2019s that simple. So please make sure to provide documentation updates for any new features you contributed, including useful example code snippets. Needless to say, as a user of this project you're perfect for helping us improve our docs. So feel free to report documentation bugs or submit documentation changes through a pull request.","title":"Contributing"},{"location":"contributing.html#guidelines","text":"First off, thanks for taking the time to contribute! The following is a set of guidelines for contributing to Predator and its packages, which are hosted in the Zooz Predator project on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.","title":"Guidelines"},{"location":"contributing.html#submitting-a-bug-report","text":"Bugs are tracked as GitHub issues . Provide the following information when submitting a bug: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. Provide specific examples to demonstrate the steps. Include links to files or GitHub projects, or copy pasteable snippets, which you use in those examples. If you're providing snippets in the issue, use Markdown code blocks. Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why.","title":"Submitting a Bug Report"},{"location":"contributing.html#contributing-code-changes","text":"As an open-source project, we welcome and encourage the community to submit patches directly to the project. In our collaborative open source environment, standards for submitting changes help reduce the chaos that can result from an active development community. So here are some high-level steps we suggest you follow when contributing code changes: Fork the project clone locally. Create an upstream remote and sync your local copy before you branch. Branch for each separate piece of work. Do the work, write good commit messages. Commit messages must adhere to commitlint standards. Push to your origin repository. Create a new PR in GitHub. Respond to any code review feedback.","title":"Contributing Code Changes"},{"location":"contributing.html#contributing-documentation-changes","text":"Documentation is mega-important. Predator cannot truly succeed without great documentation. It\u2019s that simple. So please make sure to provide documentation updates for any new features you contributed, including useful example code snippets. Needless to say, as a user of this project you're perfect for helping us improve our docs. So feel free to report documentation bugs or submit documentation changes through a pull request.","title":"Contributing Documentation Changes"},{"location":"installation.html","text":"Installing Predator # Before Predator, running two or more tests simultaneously was limited due to third party limitations. Now, you are able to scale tests using our own resources. With support for both Kubernetes and Metronome, all you need to do is provide the platform configuration, sit back, and let Predator deploy runners that load your API from your chosen platform. You're probably eager to get your hands dirty, so let's go ahead and install Predator. Kubernetes # Install Predator from the Helm Hub DC/OS # Predator can be installed through DC/OS Universe within the cluster. For examples and more info check Universe Catalog Docker # Predator runs using Docker. In order to run Predator locally, clone the repository and run the following command: runPredatorLocal.sh Or, you can run Predator without cloning the repository with the following command : docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator where $MACHINE_IP is the local ip address of your machine. After successfully mounting the Predator docker image, access Predator by typing the following URL in your browser: http://{$MACHINE_IP}/ui","title":"Installation"},{"location":"installation.html#installing-predator","text":"Before Predator, running two or more tests simultaneously was limited due to third party limitations. Now, you are able to scale tests using our own resources. With support for both Kubernetes and Metronome, all you need to do is provide the platform configuration, sit back, and let Predator deploy runners that load your API from your chosen platform. You're probably eager to get your hands dirty, so let's go ahead and install Predator.","title":"Installing Predator"},{"location":"installation.html#kubernetes","text":"Install Predator from the Helm Hub","title":"Kubernetes"},{"location":"installation.html#dcos","text":"Predator can be installed through DC/OS Universe within the cluster. For examples and more info check Universe Catalog","title":"DC/OS"},{"location":"installation.html#docker","text":"Predator runs using Docker. In order to run Predator locally, clone the repository and run the following command: runPredatorLocal.sh Or, you can run Predator without cloning the repository with the following command : docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator where $MACHINE_IP is the local ip address of your machine. After successfully mounting the Predator docker image, access Predator by typing the following URL in your browser: http://{$MACHINE_IP}/ui","title":"Docker"},{"location":"myfirsttest.html","text":"Adding your First Test # In this section, we will walk you through the steps of creating a simple test in Predator. It will allow you to familiarize yourself with some basic concepts, before moving onto some more advanced features later on. Following along with the examples To make it easy for you to follow along with the steps below, we created a demo Predator docker image. This image allows you to invoke our fictitious petstore API, used in the examples that follow. You can retrieve and run the docker image with the following command (just make sure to replace {MACHINE_IP} with the IP address of your own machine): docker run -d -p 3000:3000 --name predator-petstore zooz/predator-builds:petstore Adding a new test is easy. From the Predator web interface, choose Tests View Tests . Then click Create Test and complete all fields on the form, like so: Test Scenarios # Now proceed to add one or more test scenarios . A scenario is a sequence of HTTP requests aimed to test the performance of a single piece of functionality. For instance, a scenario could test the performance of multiple users requesting information about a specific pet simultaneously. Another scenario could be ordering a pet from the pet store. To add a new scenario, create a new test or edit an existing one. Then click the scenario button and do the following: Set a scenario weight . Allows for the probability of a scenario being picked by a new virtual user to be \"weighed\" relative to other scenarios. If not specified, each scenario is equally likely to be picked. Click the Steps button to add scenario steps . This allows you to add the . Here's a sample scenario that fetches the inventory from the pet store: Pre-scenario Requests # Sometimes a prerequisite must be fulfilled before your scenarios can actually work. For example, pets must already have been created before you can fetch them from the inventory. This is where a pre-scenario request comes into play. A pre-scenario request is an HTTP request that Predator executes before running the scenarios in your test. To add a pre-scenario request, create a new test or edit an existing one. Then click the before button and add all request specifications. HTTP Request Properties # When adding scenario steps or pre-scenario requests, you will need to define the properties of the HTTP request that will be invoked: While most of the properties are self-explanatory, the following items may require some additional explanation: gzip : This will compress the request body. forever : Indicates whether the connection to the endpoint will be kept alive. Captures : Allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. See Data Reuse with Variables . Running the Test # With all scenarios in place, let's go ahead and run the test. Predator executes tests through so-called jobs . To create a job, choose Tests View tests . Then click RUN for the test you want to execute and complete all fields in the Create new job dialog. When done, click SUBMIT . Depending on your configuration, the job will either execute immediately or at scheduled intervals. Note To see the intervals at which your jobs will run, see Scheduled Tasks . The following table explains the job parameters you can configure: Setting Description Notes Free text describing the job. Arrival rate The number of times per second that the test scenarios will run. Duration (seconds) The time during which the test will run. In seconds. Ramp to Used in combination with the arrival rate and duration values. Increases the arrival rate linearly to the value specified, within the specified duration. Parallelism The number of runners that will be allocated to executing the test. The arrival rate , duration and Max virtual users will be split between the specified number of runners. Max virtual users The maximum number of virtual users executing the scenario requests. This places a threshold on the number of requests that can exist simultaneously. Environment Free text describing the environment against which the test is executed. Cron expression A cron expression for scheduling the test to run periodically at a specified date/time. Run immediately Determines if the test will be executed immediately when the job is saved. New Email An email address to which Predator will send a message when the test execution is completed. New Webhook A URL to which an event will be sent when the test execution is completed. The event body will include detailed information about the test, such as the number of scenarios that were executed and the number of requests that were invoked. Viewing the Test Report # Your curiosity is probably reaching an all-time high right now, as Predator is working hard to push your API to its limits. So how's your API performing under all that pressure? Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view a detailed report of the test that was executed. For more information about the data shown in the test reports, see Test Reports .","title":"My First Test"},{"location":"myfirsttest.html#adding-your-first-test","text":"In this section, we will walk you through the steps of creating a simple test in Predator. It will allow you to familiarize yourself with some basic concepts, before moving onto some more advanced features later on. Following along with the examples To make it easy for you to follow along with the steps below, we created a demo Predator docker image. This image allows you to invoke our fictitious petstore API, used in the examples that follow. You can retrieve and run the docker image with the following command (just make sure to replace {MACHINE_IP} with the IP address of your own machine): docker run -d -p 3000:3000 --name predator-petstore zooz/predator-builds:petstore Adding a new test is easy. From the Predator web interface, choose Tests View Tests . Then click Create Test and complete all fields on the form, like so:","title":"Adding your First Test"},{"location":"myfirsttest.html#test-scenarios","text":"Now proceed to add one or more test scenarios . A scenario is a sequence of HTTP requests aimed to test the performance of a single piece of functionality. For instance, a scenario could test the performance of multiple users requesting information about a specific pet simultaneously. Another scenario could be ordering a pet from the pet store. To add a new scenario, create a new test or edit an existing one. Then click the scenario button and do the following: Set a scenario weight . Allows for the probability of a scenario being picked by a new virtual user to be \"weighed\" relative to other scenarios. If not specified, each scenario is equally likely to be picked. Click the Steps button to add scenario steps . This allows you to add the . Here's a sample scenario that fetches the inventory from the pet store:","title":"Test Scenarios"},{"location":"myfirsttest.html#pre-scenario-requests","text":"Sometimes a prerequisite must be fulfilled before your scenarios can actually work. For example, pets must already have been created before you can fetch them from the inventory. This is where a pre-scenario request comes into play. A pre-scenario request is an HTTP request that Predator executes before running the scenarios in your test. To add a pre-scenario request, create a new test or edit an existing one. Then click the before button and add all request specifications.","title":"Pre-scenario Requests"},{"location":"myfirsttest.html#http-request-properties","text":"When adding scenario steps or pre-scenario requests, you will need to define the properties of the HTTP request that will be invoked: While most of the properties are self-explanatory, the following items may require some additional explanation: gzip : This will compress the request body. forever : Indicates whether the connection to the endpoint will be kept alive. Captures : Allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. See Data Reuse with Variables .","title":"HTTP Request Properties"},{"location":"myfirsttest.html#running-the-test","text":"With all scenarios in place, let's go ahead and run the test. Predator executes tests through so-called jobs . To create a job, choose Tests View tests . Then click RUN for the test you want to execute and complete all fields in the Create new job dialog. When done, click SUBMIT . Depending on your configuration, the job will either execute immediately or at scheduled intervals. Note To see the intervals at which your jobs will run, see Scheduled Tasks . The following table explains the job parameters you can configure: Setting Description Notes Free text describing the job. Arrival rate The number of times per second that the test scenarios will run. Duration (seconds) The time during which the test will run. In seconds. Ramp to Used in combination with the arrival rate and duration values. Increases the arrival rate linearly to the value specified, within the specified duration. Parallelism The number of runners that will be allocated to executing the test. The arrival rate , duration and Max virtual users will be split between the specified number of runners. Max virtual users The maximum number of virtual users executing the scenario requests. This places a threshold on the number of requests that can exist simultaneously. Environment Free text describing the environment against which the test is executed. Cron expression A cron expression for scheduling the test to run periodically at a specified date/time. Run immediately Determines if the test will be executed immediately when the job is saved. New Email An email address to which Predator will send a message when the test execution is completed. New Webhook A URL to which an event will be sent when the test execution is completed. The event body will include detailed information about the test, such as the number of scenarios that were executed and the number of requests that were invoked.","title":"Running the Test"},{"location":"myfirsttest.html#viewing-the-test-report","text":"Your curiosity is probably reaching an all-time high right now, as Predator is working hard to push your API to its limits. So how's your API performing under all that pressure? Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view a detailed report of the test that was executed. For more information about the data shown in the test reports, see Test Reports .","title":"Viewing the Test Report"},{"location":"plugins.html","text":"Plugins # Metrics and test reports are an essential part of Predator. For this, Predator can be integrated to send test metrics to either Prometheus or Influx, and to send final test results straight to your email upon test completion. SMTP Server - Email Notifier # Prometheus # InfluxDB #","title":"Plugins"},{"location":"plugins.html#plugins","text":"Metrics and test reports are an essential part of Predator. For this, Predator can be integrated to send test metrics to either Prometheus or Influx, and to send final test results straight to your email upon test completion.","title":"Plugins"},{"location":"plugins.html#smtp-server-email-notifier","text":"","title":"SMTP Server - Email Notifier"},{"location":"plugins.html#prometheus","text":"","title":"Prometheus"},{"location":"plugins.html#influxdb","text":"","title":"InfluxDB"},{"location":"schedulesandreports.html","text":"Scheduled Tasks # Predator creates scheduled tasks for all tests that you scheduled to run at predefined intervals. To see the scheduled tasks that are registered in the system, choose Tasks Scheduled Tasks Test Reports # Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view detailed reports of the tests that were executed.","title":"Schedules and Reports"},{"location":"schedulesandreports.html#scheduled-tasks","text":"Predator creates scheduled tasks for all tests that you scheduled to run at predefined intervals. To see the scheduled tasks that are registered in the system, choose Tasks Scheduled Tasks","title":"Scheduled Tasks"},{"location":"schedulesandreports.html#test-reports","text":"Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view detailed reports of the tests that were executed.","title":"Test Reports"},{"location":"tests.html","text":"What you should already know We assume you have already familiarized yourself with the basic concepts used in Predator and that you successfully created your first test. If not, we strongly recommend you first complete the steps in the My First Test topic before proceeding with the instructions below. Data Reuse with Variables # The Captures field of the HTTP request properties allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. Use JSONPath syntax to extract the data of your choice. In the following example we extract the id field and store it in a petId variable: You can then use petId in a request by placing it between double brackets, like so: {{petId}} Here's an example of using {{petId}} in the request path: Request Reuse with DSL Definitions # This is the moment where Predator shows its teeth and unleashes its true power. Writing a performance test that checks specific parts of your API end-to-end can be a huge hassle, but now it is effortless. By creating DSL definitions using Predator's Domain Specific Language (DSL), request templates are generated which you can then reuse in the same test and in other tests under the same DSL type, reducing replication. Let's dive right in and get going with our first DSL definition. Predator API This functionality is only available through the Predator API . Creating a DSL Definition # Before you can use a DSL definition, you must create it first. You do so by invoking the Create DSL Definition request. Here's an example request body for creating a DSL definition of a GET request. Notice how we use the {{petId}} in the url endpoint (we will create this variable in the example of a POST request DSL definition): { name : get-pet , request : { get : { url : http://127.0.0.1:3000/pets/{{petId}} } } } The request body for creating a DSL definition of a POST request is a bit more elaborate, since it requires that you pass in the entire body that makes up the POST request. The following example shows how to do this. Notice how we add a capture array, in which we define the petId variable for storing the pet ID. We can then reuse it in another request (like in the example of a GET request above). { name : create-pet , request : { post : { url : http://127.0.0.1:3000/pets , json : { name : mickey , species : dog }, capture : [ { json : $.id , as : petId } ] } } } Creating a Test that Uses the DSL # Tests that use a DSL definition can only be created using the Create Test API request. The Create Test API request body must include all components that make up a test, including pre-scenario requests and scenarios. However, instead of defining the entire HTTP request in each scenario step (as you would through the Predator UI), you can now reference the HTTP request through its DSL definition. You do so, using the action property (in the steps array). Here's an example: { name : Pet store , description : DSL , type : dsl , // Make sure the type is set to DSL before : { steps : [ { action : petstore.create-pet } ] }, scenarios : [ { scenario_name : Only get pet , steps : [ { action : petstore.get-pet } ] } ] } There are two additional items to note: The type must always be set to dsl . The action value uses the following syntax: {dsl_group}.{dsl_name} , in which the dsl_group is the name used in the path of the Create DSL Definition API request. If you login to the Predator UI after creating the test, you will notice that the test has been added with a type of dsl . You can now run the test as you would any other. Creating a Test with Custom Logic in Javascript # Tests can use custom Javascript functions. To use custom Javascript functions in your tests, first create a text file holding your Javascript code and upload it to public repository (such as S3, Dropbox, Google Docs etc.). Then create the test, passing the processor_file_url field in the request body to inform Predator of the URL path to your Javascript file. Predator will download and save this file (any changes done to your Javascript code after creating the test will not be reflected). Artillery guidelines To include Javascript functions in your scenarios, make sure to follow the Artillery guidelines . Here's an example of a test using custom Javascript code: { name : custom logic in Javascript example , description : custom logic in Javascript , processor_file_url : https://www.dropbox.com/s/yourFilePath/fileName.txt?dl=1 , type : basic , scenarios : [ { name : Processor exmaple , flow : [ { function : generateRandomDataGlobal }, { get : { url : http://www.google.com/{{ randomPath }} , afterResponse : logResponse } }, { log : ********************* Sent a request to /users with {{ name }}, {{ email }}, {{ password }} } ] } ] } Here's some sample Javascript code: use strict ; const uuid = require ( uuid/v4 ); module . exports = { logResponse , generateRandomDataGlobal }; function logResponse ( requestParams , response , context , ee , next ) { if ( response . statusCode !== 200 ){ console . log ( **************** fail with status code: + JSON . stringify ( response . statusCode )); console . log ( **************** host is: + JSON . stringify ( response . request . uri . host )); console . log ( **************** path is: + JSON . stringify ( response . request . uri . pathname )); console . log ( **************** response headers is: + JSON . stringify ( response . headers )); } return next (); // MUST be called for the scenario to continue } function generateRandomDataGlobal ( userContext , events , done ) { console . log ( userContext is: + JSON . stringify ( userContext )); userContext . vars . randomPath = random_path_ + uuid (); userContext . vars . name = name_random_ + uuid (); userContext . vars . email = email_random_ + uuid (); userContext . vars . password = password_random_ + uuid (); return done (); } In the example above, we created a test with 2 Javascript functions: logResponse() will log the result received, if the statusCode is different from 200. The function will be called after each response using the afterResponse command. generateRandomDataGlobal() is a global function that creates global variables. Those variables can be used in the scope of the test by using the {{ }} syntax.","title":"Advanced Test Setup"},{"location":"tests.html#data-reuse-with-variables","text":"The Captures field of the HTTP request properties allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. Use JSONPath syntax to extract the data of your choice. In the following example we extract the id field and store it in a petId variable: You can then use petId in a request by placing it between double brackets, like so: {{petId}} Here's an example of using {{petId}} in the request path:","title":"Data Reuse with Variables"},{"location":"tests.html#request-reuse-with-dsl-definitions","text":"This is the moment where Predator shows its teeth and unleashes its true power. Writing a performance test that checks specific parts of your API end-to-end can be a huge hassle, but now it is effortless. By creating DSL definitions using Predator's Domain Specific Language (DSL), request templates are generated which you can then reuse in the same test and in other tests under the same DSL type, reducing replication. Let's dive right in and get going with our first DSL definition. Predator API This functionality is only available through the Predator API .","title":"Request Reuse with DSL Definitions"},{"location":"tests.html#creating-a-dsl-definition","text":"Before you can use a DSL definition, you must create it first. You do so by invoking the Create DSL Definition request. Here's an example request body for creating a DSL definition of a GET request. Notice how we use the {{petId}} in the url endpoint (we will create this variable in the example of a POST request DSL definition): { name : get-pet , request : { get : { url : http://127.0.0.1:3000/pets/{{petId}} } } } The request body for creating a DSL definition of a POST request is a bit more elaborate, since it requires that you pass in the entire body that makes up the POST request. The following example shows how to do this. Notice how we add a capture array, in which we define the petId variable for storing the pet ID. We can then reuse it in another request (like in the example of a GET request above). { name : create-pet , request : { post : { url : http://127.0.0.1:3000/pets , json : { name : mickey , species : dog }, capture : [ { json : $.id , as : petId } ] } } }","title":"Creating a DSL Definition"},{"location":"tests.html#creating-a-test-that-uses-the-dsl","text":"Tests that use a DSL definition can only be created using the Create Test API request. The Create Test API request body must include all components that make up a test, including pre-scenario requests and scenarios. However, instead of defining the entire HTTP request in each scenario step (as you would through the Predator UI), you can now reference the HTTP request through its DSL definition. You do so, using the action property (in the steps array). Here's an example: { name : Pet store , description : DSL , type : dsl , // Make sure the type is set to DSL before : { steps : [ { action : petstore.create-pet } ] }, scenarios : [ { scenario_name : Only get pet , steps : [ { action : petstore.get-pet } ] } ] } There are two additional items to note: The type must always be set to dsl . The action value uses the following syntax: {dsl_group}.{dsl_name} , in which the dsl_group is the name used in the path of the Create DSL Definition API request. If you login to the Predator UI after creating the test, you will notice that the test has been added with a type of dsl . You can now run the test as you would any other.","title":"Creating a Test that Uses the DSL"},{"location":"tests.html#creating-a-test-with-custom-logic-in-javascript","text":"Tests can use custom Javascript functions. To use custom Javascript functions in your tests, first create a text file holding your Javascript code and upload it to public repository (such as S3, Dropbox, Google Docs etc.). Then create the test, passing the processor_file_url field in the request body to inform Predator of the URL path to your Javascript file. Predator will download and save this file (any changes done to your Javascript code after creating the test will not be reflected). Artillery guidelines To include Javascript functions in your scenarios, make sure to follow the Artillery guidelines . Here's an example of a test using custom Javascript code: { name : custom logic in Javascript example , description : custom logic in Javascript , processor_file_url : https://www.dropbox.com/s/yourFilePath/fileName.txt?dl=1 , type : basic , scenarios : [ { name : Processor exmaple , flow : [ { function : generateRandomDataGlobal }, { get : { url : http://www.google.com/{{ randomPath }} , afterResponse : logResponse } }, { log : ********************* Sent a request to /users with {{ name }}, {{ email }}, {{ password }} } ] } ] } Here's some sample Javascript code: use strict ; const uuid = require ( uuid/v4 ); module . exports = { logResponse , generateRandomDataGlobal }; function logResponse ( requestParams , response , context , ee , next ) { if ( response . statusCode !== 200 ){ console . log ( **************** fail with status code: + JSON . stringify ( response . statusCode )); console . log ( **************** host is: + JSON . stringify ( response . request . uri . host )); console . log ( **************** path is: + JSON . stringify ( response . request . uri . pathname )); console . log ( **************** response headers is: + JSON . stringify ( response . headers )); } return next (); // MUST be called for the scenario to continue } function generateRandomDataGlobal ( userContext , events , done ) { console . log ( userContext is: + JSON . stringify ( userContext )); userContext . vars . randomPath = random_path_ + uuid (); userContext . vars . name = name_random_ + uuid (); userContext . vars . email = email_random_ + uuid (); userContext . vars . password = password_random_ + uuid (); return done (); } In the example above, we created a test with 2 Javascript functions: logResponse() will log the result received, if the statusCode is different from 200. The function will be called after each response using the afterResponse command. generateRandomDataGlobal() is a global function that creates global variables. Those variables can be used in the scope of the test by using the {{ }} syntax.","title":"Creating a Test with Custom Logic in Javascript"}]}