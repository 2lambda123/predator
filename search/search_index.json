{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"about.html","text":"Introducing Predator # Distribute open source performance testing platform for APIs. Requests per second, request latency, and overall system performance and reliability are some fundamental concepts that need to be taken into account when designing a high capacity API. With CI/CD becoming a common deployment methodology, deployments to production are a constant occurrence. So how can you ensure that the core capabilities you initially designed your system to have remain intact? The answer is: use Predator! Predator is a performance platform that can be configured to automatically load your API with scheduled tests that provide in-depth performance metrics. While sipping on your (morning) coffee, you can simply check the test summaries to make sure your system is performing as expected. By comparing test results to results from previous test runs, you can easily identify system-wide regressions. Predator\u2019s integration with custom dashboards, test metrics, and service logs make pinpointing performance bugs painless,especially when you check up on the system on a daily basis. Main Features # Distributed load : Predator supports an unlimited number of load generators that produce multiple load runners concurrently. Real time reports : Predator aggregates all concurrent runs into a single beautiful report in real time (latency, rps, status codes and more). Built for the cloud : Predator is built to take advantage of Kubernetes and DC/OS. It's integrated with those platforms and can manage the load generators lifecycles by itself. One click installation : Predator can be installed with just one click in Kubernetes and DC/OS, or on any other machine running Docker. Supports 5 Different databases : Predator provides out-of-the box functionality for persisting data in Postgres, MySQL, MSSQL and SQLITE. Scheduled jobs : Predator can run recurring tests using cron expressions. 3rd partry metrics : Predator comes integrated with Prometheus and Influx. Simply configure it through the predator REST API or using the UI. Rich UI : Predator offers a rich UI along with a powerful REST API. Based on artillery.io : Predator uses artillery as its load engine to fire the requests. The schema for creating tests via the Predator REST API is based on the artillery schema. UI # Under the hood # Feature Comparison #","title":"About"},{"location":"about.html#introducing-predator","text":"Distribute open source performance testing platform for APIs. Requests per second, request latency, and overall system performance and reliability are some fundamental concepts that need to be taken into account when designing a high capacity API. With CI/CD becoming a common deployment methodology, deployments to production are a constant occurrence. So how can you ensure that the core capabilities you initially designed your system to have remain intact? The answer is: use Predator! Predator is a performance platform that can be configured to automatically load your API with scheduled tests that provide in-depth performance metrics. While sipping on your (morning) coffee, you can simply check the test summaries to make sure your system is performing as expected. By comparing test results to results from previous test runs, you can easily identify system-wide regressions. Predator\u2019s integration with custom dashboards, test metrics, and service logs make pinpointing performance bugs painless,especially when you check up on the system on a daily basis.","title":"Introducing Predator"},{"location":"about.html#main-features","text":"Distributed load : Predator supports an unlimited number of load generators that produce multiple load runners concurrently. Real time reports : Predator aggregates all concurrent runs into a single beautiful report in real time (latency, rps, status codes and more). Built for the cloud : Predator is built to take advantage of Kubernetes and DC/OS. It's integrated with those platforms and can manage the load generators lifecycles by itself. One click installation : Predator can be installed with just one click in Kubernetes and DC/OS, or on any other machine running Docker. Supports 5 Different databases : Predator provides out-of-the box functionality for persisting data in Postgres, MySQL, MSSQL and SQLITE. Scheduled jobs : Predator can run recurring tests using cron expressions. 3rd partry metrics : Predator comes integrated with Prometheus and Influx. Simply configure it through the predator REST API or using the UI. Rich UI : Predator offers a rich UI along with a powerful REST API. Based on artillery.io : Predator uses artillery as its load engine to fire the requests. The schema for creating tests via the Predator REST API is based on the artillery schema.","title":"Main Features"},{"location":"about.html#ui","text":"","title":"UI"},{"location":"about.html#under-the-hood","text":"","title":"Under the hood"},{"location":"about.html#feature-comparison","text":"","title":"Feature Comparison"},{"location":"apireference.html","text":"The Predator API gives you advanced control over the tests you create and allows you utilize functionalities not available through the Predator UI. For instance, using the API you can create DSL definitions and reuse them in your requests. Go ahead and dig into our Predator API documentation .","title":"API Reference"},{"location":"benchmarks.html","text":"Benchmarks # Supported from version zooz/predator:1.3.0 Motivation # Load test results increase in their value and importance when compared to a certain benchmark. By creating a benchmark for a specific test, each subsequent test run for that test will be given a score from 0-100 summarizing the test run in one simple to analyze numerical value. The benchmarks feature alongside the compare test runs are tools that leverage predators reports and ensures that new releases stand up to your performance requirements. Setting Up # Test as a Benchmark # To set a test as a benchmark, choose a test run that you are satisfied with the results, open its report and click on Set as Benchmark . Now, all future runs of this test will have their score calculated based on the benchmarks selected. Benchmark Weights # There are 5 factors taken from the test run results that affect the score of the run. Median : Percentage of the score affected by median results P95 : Percentage of the score affected by p95 results Server errors : Percentage of the score affected by server errors ratio Client errors : Percentage of the score affected by client errors ratio RPS : Percentage of the score affected by requests per second results Please refer to the configuration manual for further documentation.","title":"Benchmarks"},{"location":"benchmarks.html#benchmarks","text":"Supported from version zooz/predator:1.3.0","title":"Benchmarks"},{"location":"benchmarks.html#motivation","text":"Load test results increase in their value and importance when compared to a certain benchmark. By creating a benchmark for a specific test, each subsequent test run for that test will be given a score from 0-100 summarizing the test run in one simple to analyze numerical value. The benchmarks feature alongside the compare test runs are tools that leverage predators reports and ensures that new releases stand up to your performance requirements.","title":"Motivation"},{"location":"benchmarks.html#setting-up","text":"","title":"Setting Up"},{"location":"benchmarks.html#test-as-a-benchmark","text":"To set a test as a benchmark, choose a test run that you are satisfied with the results, open its report and click on Set as Benchmark . Now, all future runs of this test will have their score calculated based on the benchmarks selected.","title":"Test as a Benchmark"},{"location":"benchmarks.html#benchmark-weights","text":"There are 5 factors taken from the test run results that affect the score of the run. Median : Percentage of the score affected by median results P95 : Percentage of the score affected by p95 results Server errors : Percentage of the score affected by server errors ratio Client errors : Percentage of the score affected by client errors ratio RPS : Percentage of the score affected by requests per second results Please refer to the configuration manual for further documentation.","title":"Benchmark Weights"},{"location":"configuration.html","text":"Configuration # When running Predator, it is possible to retrieve and update some of the service's configuration during runtime with the /config endpoint, please check the API reference for more details. Below are variables Predator can be configured with. General # Environment Variable Configuration key Description Configurable from UI/API Default value INTERNAL_ADDRESS internal_address The local ip address of your machine \u2713 RUNNER_DOCKER_IMAGE runner_docker_image The predator-runner docker image that will run the test \u2713 zooz/predator-runner:$LATEST_TAGGED_VERSION RUNNER_CPU runner_cpu Number of CPU use by the each runner \u2713 1 RUNNER_MEMORY runner_memory Max memory to use by each runner \u2713 256 DEFAULT_EMAIL_ADDRESS default_email_address Default email to send final report to, address can be configured \u2713 ALLOW_INSECURE_TLS allow_insecure_tls If true, don't fail requests on unverified server certificate errors \u2713 false DELAY_RUNNER_MS delay_runner_ms Delay the predator runner from sending http requests (ms) \u2713 INTERVAL_CLEANUP_FINISHED_CONTAINERS_MS interval_cleanup_finished_containers_ms Interval (in ms) to search and delete finished tests containers. Value of 0 means no auto clearing enabled \u2713 0 CUSTOM_RUNNER_DEFINITION custom_runner_definition Custom json that will be merged with the kubernetes/metronome predator runner job definition. See FAQ for usage examples. API STREAMING_EXCLUDED_ATTRIBUTES streaming_excluded_attributes Attribute names to exclude from being produced in the resource to streaming platform API MAX_UPLOAD_FILE_SIZE_MB Maximum file size (in megabytes) that is allowed for uploading CSV files 10 Note RUNNER_DOCKER_IMAGE ( zooz/predator-runner:$TAGGED_VERSION ) should match the Predator's version running in order to be fully compatible with all features. Database # Environment Variable Description Configurable from UI/API Default value DATABASE_TYPE Database to integrate Predator with [Postgres, MySQL, MSSQL, SQLITE] x SQLITE DATABASE_NAME Database/Keyspace name x DATABASE_ADDRESS Database address x DATABASE_USERNAME Database username x DATABASE_PASSWORD Database password x Additional parameters for the following chosen databases: SQLITE # Environment Variable Description Configurable from UI/API Default value SQLITE_STORAGE SQLITE file name x predator Deployment # Environment Variable Description Configurable from UI/API Default value JOB_PLATFORM Type of platform using to run predator (METRONOME,KUBERNETES,DOCKER) x DOCKER Kubernetes # Environment Variable Description Configurable from UI/API Default value KUBERNETES_URL Kubernetes API Url x KUBERNETES_TOKEN Kubernetes API Token x KUBERNETES_NAMESPACE Kubernetes Namespace x Metronome # Environment Variable Description Configurable from UI/API Default value METRONOME_URL Metronome API Url x METRONOME_TOKEN Metronome API Token x Docker # Environment Variable Description Configurable from UI/API Default value DOCKER_HOST Docker engine url (host and port number of docker engine) x DOCKER_CERT_PATH Path to CA certificate directory x Streaming # Environment Variable Description Configurable from UI/API Default value STREAMING_PLATFORM Type of platform to produce messages to x N/A STREAMING_PLATFORM_HEALTH_CHECK_TIMEOUT_MS Health check timeout to streaming platform x 2000 Kafka # Applicable when STREAMING_PLATFORM = Kafka Environment Variable Description Configurable from UI/API Default value Required KAFKA_CLIENT_ID Id of kafka client x N/A x KAFKA_BROKERS String list of kafka brokers, split by ',' delimiter x N/A \u2713 KAFKA_TOPIC Topic name x N/A \u2713 KAFKA_ALLOW_AUTO_TOPIC_CREATION Enable kafka client to auto create topic if it does not exist x false x KAFKA_ADMIN_RETRIES Admin client retries x 2 x Benchmarks # Environment Variable Configuration key Description Configurable from UI/API Default value BENCHMARK_THRESHOLD benchmark_threshold Minimum acceptable score of tests, if a score is less than this value, a webhook will be sent to the threshold webhook url \u2713 benchmark_weights.percentile_ninety_five.percentage Percentage of the score affected by p95 results \u2713 20 benchmark_weights.percentile_fifty.percentage Percentage of the score affected by median results \u2713 20 benchmark_weights.server_errors_ratio.percentage Percentage of the score affected by server errors ratio \u2713 20 benchmark_weights.client_errors_ratio.percentage Percentage of the score affected by client errors ratio \u2713 20 benchmark_weights.rps.percentage Percentage of the score affected by requests per second results \u2713 20 Metrics # PROCESS.ENV Variable Configuration key Description Configurable from UI/API Default value METRICS_PLUGIN_NAME metrics_plugin_name Metrics integration to use [prometheus,influx] \u2713 Prometheus # Environment Variable Configuration key Description Configurable from UI/API Default value prometheus_metrics.push_gateway_url Url of push gateway \u2713 prometheus_metrics.buckets_sizes Bucket sizes to configure prometheus \u2713 prometheus_metrics.labels Labels will be passed to the push gateway \u2713 InfluxDB # Environment Variable Configuration key Description Configurable from UI/API Default value influx_metrics.host Influx db host \u2713 influx_metrics.username Influx db username \u2713 influx_metrics.password Influx db password \u2713 influx_metrics.database Influx db name \u2713 SMTP Server # Environment Variable Configuration key Description Configurable from UI/API Default value SMTP_FROM smtp_server.from the 'from' email address that will be used to send emails \u2713 SMTP_HOST smtp_server.host SMTP host \u2713 SMTP_PORT smtp_server.port SMTP port number \u2713 SMTP_USERNAME smtp_server.username SMTP username \u2713 SMTP_PASSWORD smtp_server.password SMTP password \u2713 SMTP_TIMEOUT smtp_server.timeout How many milliseconds to wait for the connection to establish to SMTP server \u2713 200 SMTP_SECURE smtp_server.secure if true the connection will use TLS when connecting to server. Nodemailer SMTP options \u2713 false SMTP_REJECT_UNAUTH_CERTS smtp_server.rejectUnauthCerts should fail or succeed on unauthorized certificate \u2713 false","title":"Configuration"},{"location":"configuration.html#configuration","text":"When running Predator, it is possible to retrieve and update some of the service's configuration during runtime with the /config endpoint, please check the API reference for more details. Below are variables Predator can be configured with.","title":"Configuration"},{"location":"configuration.html#general","text":"Environment Variable Configuration key Description Configurable from UI/API Default value INTERNAL_ADDRESS internal_address The local ip address of your machine \u2713 RUNNER_DOCKER_IMAGE runner_docker_image The predator-runner docker image that will run the test \u2713 zooz/predator-runner:$LATEST_TAGGED_VERSION RUNNER_CPU runner_cpu Number of CPU use by the each runner \u2713 1 RUNNER_MEMORY runner_memory Max memory to use by each runner \u2713 256 DEFAULT_EMAIL_ADDRESS default_email_address Default email to send final report to, address can be configured \u2713 ALLOW_INSECURE_TLS allow_insecure_tls If true, don't fail requests on unverified server certificate errors \u2713 false DELAY_RUNNER_MS delay_runner_ms Delay the predator runner from sending http requests (ms) \u2713 INTERVAL_CLEANUP_FINISHED_CONTAINERS_MS interval_cleanup_finished_containers_ms Interval (in ms) to search and delete finished tests containers. Value of 0 means no auto clearing enabled \u2713 0 CUSTOM_RUNNER_DEFINITION custom_runner_definition Custom json that will be merged with the kubernetes/metronome predator runner job definition. See FAQ for usage examples. API STREAMING_EXCLUDED_ATTRIBUTES streaming_excluded_attributes Attribute names to exclude from being produced in the resource to streaming platform API MAX_UPLOAD_FILE_SIZE_MB Maximum file size (in megabytes) that is allowed for uploading CSV files 10 Note RUNNER_DOCKER_IMAGE ( zooz/predator-runner:$TAGGED_VERSION ) should match the Predator's version running in order to be fully compatible with all features.","title":"General"},{"location":"configuration.html#database","text":"Environment Variable Description Configurable from UI/API Default value DATABASE_TYPE Database to integrate Predator with [Postgres, MySQL, MSSQL, SQLITE] x SQLITE DATABASE_NAME Database/Keyspace name x DATABASE_ADDRESS Database address x DATABASE_USERNAME Database username x DATABASE_PASSWORD Database password x Additional parameters for the following chosen databases:","title":"Database"},{"location":"configuration.html#sqlite","text":"Environment Variable Description Configurable from UI/API Default value SQLITE_STORAGE SQLITE file name x predator","title":"SQLITE"},{"location":"configuration.html#deployment","text":"Environment Variable Description Configurable from UI/API Default value JOB_PLATFORM Type of platform using to run predator (METRONOME,KUBERNETES,DOCKER) x DOCKER","title":"Deployment"},{"location":"configuration.html#kubernetes","text":"Environment Variable Description Configurable from UI/API Default value KUBERNETES_URL Kubernetes API Url x KUBERNETES_TOKEN Kubernetes API Token x KUBERNETES_NAMESPACE Kubernetes Namespace x","title":"Kubernetes"},{"location":"configuration.html#metronome","text":"Environment Variable Description Configurable from UI/API Default value METRONOME_URL Metronome API Url x METRONOME_TOKEN Metronome API Token x","title":"Metronome"},{"location":"configuration.html#docker","text":"Environment Variable Description Configurable from UI/API Default value DOCKER_HOST Docker engine url (host and port number of docker engine) x DOCKER_CERT_PATH Path to CA certificate directory x","title":"Docker"},{"location":"configuration.html#streaming","text":"Environment Variable Description Configurable from UI/API Default value STREAMING_PLATFORM Type of platform to produce messages to x N/A STREAMING_PLATFORM_HEALTH_CHECK_TIMEOUT_MS Health check timeout to streaming platform x 2000","title":"Streaming"},{"location":"configuration.html#kafka","text":"Applicable when STREAMING_PLATFORM = Kafka Environment Variable Description Configurable from UI/API Default value Required KAFKA_CLIENT_ID Id of kafka client x N/A x KAFKA_BROKERS String list of kafka brokers, split by ',' delimiter x N/A \u2713 KAFKA_TOPIC Topic name x N/A \u2713 KAFKA_ALLOW_AUTO_TOPIC_CREATION Enable kafka client to auto create topic if it does not exist x false x KAFKA_ADMIN_RETRIES Admin client retries x 2 x","title":"Kafka"},{"location":"configuration.html#benchmarks","text":"Environment Variable Configuration key Description Configurable from UI/API Default value BENCHMARK_THRESHOLD benchmark_threshold Minimum acceptable score of tests, if a score is less than this value, a webhook will be sent to the threshold webhook url \u2713 benchmark_weights.percentile_ninety_five.percentage Percentage of the score affected by p95 results \u2713 20 benchmark_weights.percentile_fifty.percentage Percentage of the score affected by median results \u2713 20 benchmark_weights.server_errors_ratio.percentage Percentage of the score affected by server errors ratio \u2713 20 benchmark_weights.client_errors_ratio.percentage Percentage of the score affected by client errors ratio \u2713 20 benchmark_weights.rps.percentage Percentage of the score affected by requests per second results \u2713 20","title":"Benchmarks"},{"location":"configuration.html#metrics","text":"PROCESS.ENV Variable Configuration key Description Configurable from UI/API Default value METRICS_PLUGIN_NAME metrics_plugin_name Metrics integration to use [prometheus,influx] \u2713","title":"Metrics"},{"location":"configuration.html#prometheus","text":"Environment Variable Configuration key Description Configurable from UI/API Default value prometheus_metrics.push_gateway_url Url of push gateway \u2713 prometheus_metrics.buckets_sizes Bucket sizes to configure prometheus \u2713 prometheus_metrics.labels Labels will be passed to the push gateway \u2713","title":"Prometheus"},{"location":"configuration.html#influxdb","text":"Environment Variable Configuration key Description Configurable from UI/API Default value influx_metrics.host Influx db host \u2713 influx_metrics.username Influx db username \u2713 influx_metrics.password Influx db password \u2713 influx_metrics.database Influx db name \u2713","title":"InfluxDB"},{"location":"configuration.html#smtp-server","text":"Environment Variable Configuration key Description Configurable from UI/API Default value SMTP_FROM smtp_server.from the 'from' email address that will be used to send emails \u2713 SMTP_HOST smtp_server.host SMTP host \u2713 SMTP_PORT smtp_server.port SMTP port number \u2713 SMTP_USERNAME smtp_server.username SMTP username \u2713 SMTP_PASSWORD smtp_server.password SMTP password \u2713 SMTP_TIMEOUT smtp_server.timeout How many milliseconds to wait for the connection to establish to SMTP server \u2713 200 SMTP_SECURE smtp_server.secure if true the connection will use TLS when connecting to server. Nodemailer SMTP options \u2713 false SMTP_REJECT_UNAUTH_CERTS smtp_server.rejectUnauthCerts should fail or succeed on unauthorized certificate \u2713 false","title":"SMTP Server"},{"location":"contributing.html","text":"Guidelines # First off, thanks for taking the time to contribute! The following is a set of guidelines for contributing to Predator and its packages, which are hosted in the Zooz Predator project on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request. Submitting a Bug Report # Bugs are tracked as GitHub issues . Provide the following information when submitting a bug: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. Provide specific examples to demonstrate the steps. Include links to files or GitHub projects, or copy pasteable snippets, which you use in those examples. If you're providing snippets in the issue, use Markdown code blocks. Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. Contributing Code Changes # As an open-source project, we welcome and encourage the community to submit patches directly to the project. In our collaborative open source environment, standards for submitting changes help reduce the chaos that can result from an active development community. So here are some high-level steps we suggest you follow when contributing code changes: Fork the project & clone locally. Create an upstream remote and sync your local copy before you branch. Branch for each separate piece of work. Do the work, write good commit messages. Commit messages must adhere to commitlint standards. Push to your origin repository. Create a new PR in GitHub. Respond to any code review feedback. Contributing Documentation Changes # Documentation is mega-important. Predator cannot truly succeed without great documentation. It\u2019s that simple. So please make sure to provide documentation updates for any new features you contributed, including useful example code snippets. Needless to say, as a user of this project you're perfect for helping us improve our docs. So feel free to report documentation bugs or submit documentation changes through a pull request.","title":"Contributing"},{"location":"contributing.html#guidelines","text":"First off, thanks for taking the time to contribute! The following is a set of guidelines for contributing to Predator and its packages, which are hosted in the Zooz Predator project on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.","title":"Guidelines"},{"location":"contributing.html#submitting-a-bug-report","text":"Bugs are tracked as GitHub issues . Provide the following information when submitting a bug: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. Provide specific examples to demonstrate the steps. Include links to files or GitHub projects, or copy pasteable snippets, which you use in those examples. If you're providing snippets in the issue, use Markdown code blocks. Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why.","title":"Submitting a Bug Report"},{"location":"contributing.html#contributing-code-changes","text":"As an open-source project, we welcome and encourage the community to submit patches directly to the project. In our collaborative open source environment, standards for submitting changes help reduce the chaos that can result from an active development community. So here are some high-level steps we suggest you follow when contributing code changes: Fork the project & clone locally. Create an upstream remote and sync your local copy before you branch. Branch for each separate piece of work. Do the work, write good commit messages. Commit messages must adhere to commitlint standards. Push to your origin repository. Create a new PR in GitHub. Respond to any code review feedback.","title":"Contributing Code Changes"},{"location":"contributing.html#contributing-documentation-changes","text":"Documentation is mega-important. Predator cannot truly succeed without great documentation. It\u2019s that simple. So please make sure to provide documentation updates for any new features you contributed, including useful example code snippets. Needless to say, as a user of this project you're perfect for helping us improve our docs. So feel free to report documentation bugs or submit documentation changes through a pull request.","title":"Contributing Documentation Changes"},{"location":"faq.html","text":"Frequently Asked Questions # Installation # Predator is running, how do I access the UI? # The UI is accessible at http://$MACHINE_IP/ui where $MACHINE_IP is your local network address you used to install Predator. Tests # I run a test successfully but no report is created for the test # The Predator-Runner docker that is reporting the test results back to Predator isn't able to connect to it, which is why the test runs but no report is generated. When installing Predator in Docker, the following command is used: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 \\ -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator The INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 is what the Predator-Runner uses to communicate with Predator, and $MACHINE_IP needs to be your local network address (an IP address). You can get it by running the command: ifconfig en0 | grep 'inet ' | cut -d' ' -f2 For more information regarding correct installation of Predator using Docker visit the Installation section. It is important to note this is an issue and solution only in Docker installations. In Kubernetes and DC/OS installations the INTERNAL_ADDRESS is built in. What is the http engine Predator uses to run the load? # Predator uses Artillery as its HTTP load engine. Therefore, all basic type tests are written in Artillery syntax and all of the features Artillery supports, Predator supports. To read more about Artillery and its features visit their well written documentary: - Artillery Documentation - Artillery Basic Concepts Documentation - Artillery Test Structure Documentation - Artillery HTTP Engine Documentation I want to use Predator's API, where can I find examples for creating advanced/dsl tests? # Here is a postman collection which contains examples for creating tests with custom javascript and dsl definitions. What content-type does Predator support in its HTTP requests? # While the Predator UI currently supports creating tests only with content-type: application/json body, the actual Predator API has no such limit. When creating a test through the API, instead of specifying in the post request a json key, specify body key and change the content-type header to the appropriate content-type being used. For example: { \"post\" : { \"url\" : \"/orders\" , \"headers\" : { \"Content-Type\" : \"text/html\" }, \"body\" : \"Not Json :)\" } } In this example, the content-type used in the request is text/html and the body sent: \"Not JSON :)\" will be in that format. Configuration # I deployed Predator with Kuberenetes/Metronome but I need to customize the predator-runner job configuration (change CPU, memory, add image pull policy, etc). # Supported from version zooz/predator:1.4.0 This is possible by configuring Predator with the custom_runner_definition parameter. This is a JSON value that will be merged with the runner job definition when creating new jobs in both Kuberenetes and Metronome platforms. Example usage for Predator deployed with Kubernetes and need to change memory/cpu requests and limits while adding security context and image pull secrets: curl -X PUT \\ http://PREDATOR-API-URL/v1/config \\ -H 'Content-Type: application/json' \\ -d '{ \"custom_runner_definition\": { \"spec\": { \"template\": { \"spec\": { \"imagePullSecrets\": [{ \"name\": \"****\" }], \"securityContext\": { \"runAsUser\": 1000 }, \"containers\": [{ \"resources\": { \"requests\": { \"memory\": \"128Mi\", \"cpu\": \"0.5\" }, \"limits\": { \"memory\": \"1024Mi\", \"cpu\": \"1\" } } }] } } } } }' This will spawn each predator-runner with the desired specs. Note: this parameter is applied globally , meaning that all runners will be applied the above specs once configured. Of course, it's possible to delete this configuration if it is not needed anymore by applying the DELETE method on the /v1/config/custom_runner_definition API. I ran Predator with SQLITE and would like to migrate now to a different database. How do I do this? # Migration between different databases is not possible. In order to run Predator with a different supported database, you must restart Predator with the new configuration . Metrics # Does Predator support exporting metrics to external time series databases? # Yes, Predator has integration with both Prometheus and InfluxDB , and can export metrics by test endpoints and status codes, something that is currently is not available to configure in the Predator UI, but is supported through the API. How can I export metrics to Prometheus? # Prometheus by its nature is a scraper, while the Predator-Runner is a job without a specific API and finite time life. To overcome this: 1. Deploy Prometheus 2. Deploy Prometheus push gateway 3. Configure Predator to push metrics to the push gateway. 4. Configure Prometheus to scrape from the push gateway. Is there any dashboard for Grafana I can use? # This dashboard for example configured to read from Prometheus.","title":"FAQ"},{"location":"faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq.html#installation","text":"","title":"Installation"},{"location":"faq.html#predator-is-running-how-do-i-access-the-ui","text":"The UI is accessible at http://$MACHINE_IP/ui where $MACHINE_IP is your local network address you used to install Predator.","title":"Predator is running, how do I access the UI?"},{"location":"faq.html#tests","text":"","title":"Tests"},{"location":"faq.html#i-run-a-test-successfully-but-no-report-is-created-for-the-test","text":"The Predator-Runner docker that is reporting the test results back to Predator isn't able to connect to it, which is why the test runs but no report is generated. When installing Predator in Docker, the following command is used: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 \\ -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator The INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 is what the Predator-Runner uses to communicate with Predator, and $MACHINE_IP needs to be your local network address (an IP address). You can get it by running the command: ifconfig en0 | grep 'inet ' | cut -d' ' -f2 For more information regarding correct installation of Predator using Docker visit the Installation section. It is important to note this is an issue and solution only in Docker installations. In Kubernetes and DC/OS installations the INTERNAL_ADDRESS is built in.","title":"I run a test successfully but no report is created for the test"},{"location":"faq.html#what-is-the-http-engine-predator-uses-to-run-the-load","text":"Predator uses Artillery as its HTTP load engine. Therefore, all basic type tests are written in Artillery syntax and all of the features Artillery supports, Predator supports. To read more about Artillery and its features visit their well written documentary: - Artillery Documentation - Artillery Basic Concepts Documentation - Artillery Test Structure Documentation - Artillery HTTP Engine Documentation","title":"What is the http engine Predator uses to run the load?"},{"location":"faq.html#i-want-to-use-predators-api-where-can-i-find-examples-for-creating-advanceddsl-tests","text":"Here is a postman collection which contains examples for creating tests with custom javascript and dsl definitions.","title":"I want to use Predator's API, where can I find examples for creating advanced/dsl tests?"},{"location":"faq.html#what-content-type-does-predator-support-in-its-http-requests","text":"While the Predator UI currently supports creating tests only with content-type: application/json body, the actual Predator API has no such limit. When creating a test through the API, instead of specifying in the post request a json key, specify body key and change the content-type header to the appropriate content-type being used. For example: { \"post\" : { \"url\" : \"/orders\" , \"headers\" : { \"Content-Type\" : \"text/html\" }, \"body\" : \"Not Json :)\" } } In this example, the content-type used in the request is text/html and the body sent: \"Not JSON :)\" will be in that format.","title":"What content-type does Predator support in its HTTP requests?"},{"location":"faq.html#configuration","text":"","title":"Configuration"},{"location":"faq.html#i-deployed-predator-with-kuberenetesmetronome-but-i-need-to-customize-the-predator-runner-job-configuration-change-cpu-memory-add-image-pull-policy-etc","text":"Supported from version zooz/predator:1.4.0 This is possible by configuring Predator with the custom_runner_definition parameter. This is a JSON value that will be merged with the runner job definition when creating new jobs in both Kuberenetes and Metronome platforms. Example usage for Predator deployed with Kubernetes and need to change memory/cpu requests and limits while adding security context and image pull secrets: curl -X PUT \\ http://PREDATOR-API-URL/v1/config \\ -H 'Content-Type: application/json' \\ -d '{ \"custom_runner_definition\": { \"spec\": { \"template\": { \"spec\": { \"imagePullSecrets\": [{ \"name\": \"****\" }], \"securityContext\": { \"runAsUser\": 1000 }, \"containers\": [{ \"resources\": { \"requests\": { \"memory\": \"128Mi\", \"cpu\": \"0.5\" }, \"limits\": { \"memory\": \"1024Mi\", \"cpu\": \"1\" } } }] } } } } }' This will spawn each predator-runner with the desired specs. Note: this parameter is applied globally , meaning that all runners will be applied the above specs once configured. Of course, it's possible to delete this configuration if it is not needed anymore by applying the DELETE method on the /v1/config/custom_runner_definition API.","title":"I deployed Predator with Kuberenetes/Metronome but I need to customize the predator-runner job configuration (change CPU, memory, add image pull policy, etc)."},{"location":"faq.html#i-ran-predator-with-sqlite-and-would-like-to-migrate-now-to-a-different-database-how-do-i-do-this","text":"Migration between different databases is not possible. In order to run Predator with a different supported database, you must restart Predator with the new configuration .","title":"I ran Predator with SQLITE and would like to migrate now to a different database. How do I do this?"},{"location":"faq.html#metrics","text":"","title":"Metrics"},{"location":"faq.html#does-predator-support-exporting-metrics-to-external-time-series-databases","text":"Yes, Predator has integration with both Prometheus and InfluxDB , and can export metrics by test endpoints and status codes, something that is currently is not available to configure in the Predator UI, but is supported through the API.","title":"Does Predator support exporting metrics to external time series databases?"},{"location":"faq.html#how-can-i-export-metrics-to-prometheus","text":"Prometheus by its nature is a scraper, while the Predator-Runner is a job without a specific API and finite time life. To overcome this: 1. Deploy Prometheus 2. Deploy Prometheus push gateway 3. Configure Predator to push metrics to the push gateway. 4. Configure Prometheus to scrape from the push gateway.","title":"How can I export metrics to Prometheus?"},{"location":"faq.html#is-there-any-dashboard-for-grafana-i-can-use","text":"This dashboard for example configured to read from Prometheus.","title":"Is there any dashboard for Grafana I can use?"},{"location":"functionalandassertions.html","text":"Functional Tests and Assertions # Supported from version zooz/predator:1.5.0 Load tests are important to stress the system and see how it is dealing with the stress. The actual functionality and acceptance tests are not usually tested in load tests. For this, Predator offers running functional tests with assertions . Creating a Functional Tests # Functional tests are created in the same manner as the load tests. When creating functional tests, it's advised to add expectations so that you will be informed in the report if the acceptance tests passed or not. These expectations will be checked for each response received in the test and the report will display the assertion results. Expectations # The possible expectations that can be created to assert the response received: statusCode contentType hasProperty hasHeader equals matchesRegexp Example # A simple functional test that sends a GET request to http://www.google.com/ and asserts that the response received has: statusCode: 200 contentType: application/json hasProperty: body Running a Functional Tests # In order to run the created test as a functional test and not a load test, the only difference is the actual load we run. For this we need to create a job with type functional_test and arrival_count as the rate parameter (instead of arrival_rate in load_tests). This means that for the test duration, the test will send the amount of requests that are configured in the arrival_count param. Example : A test duration of 10 minutes with arrival_count = 600 will result in 1 request per second.","title":"Functional Tests and Assertions"},{"location":"functionalandassertions.html#functional-tests-and-assertions","text":"Supported from version zooz/predator:1.5.0 Load tests are important to stress the system and see how it is dealing with the stress. The actual functionality and acceptance tests are not usually tested in load tests. For this, Predator offers running functional tests with assertions .","title":"Functional Tests and Assertions"},{"location":"functionalandassertions.html#creating-a-functional-tests","text":"Functional tests are created in the same manner as the load tests. When creating functional tests, it's advised to add expectations so that you will be informed in the report if the acceptance tests passed or not. These expectations will be checked for each response received in the test and the report will display the assertion results.","title":"Creating a Functional Tests"},{"location":"functionalandassertions.html#expectations","text":"The possible expectations that can be created to assert the response received: statusCode contentType hasProperty hasHeader equals matchesRegexp","title":"Expectations"},{"location":"functionalandassertions.html#example","text":"A simple functional test that sends a GET request to http://www.google.com/ and asserts that the response received has: statusCode: 200 contentType: application/json hasProperty: body","title":"Example"},{"location":"functionalandassertions.html#running-a-functional-tests","text":"In order to run the created test as a functional test and not a load test, the only difference is the actual load we run. For this we need to create a job with type functional_test and arrival_count as the rate parameter (instead of arrival_rate in load_tests). This means that for the test duration, the test will send the amount of requests that are configured in the arrival_count param. Example : A test duration of 10 minutes with arrival_count = 600 will result in 1 request per second.","title":"Running a Functional Tests"},{"location":"installation.html","text":"Installing Predator # Before Predator, running two or more tests simultaneously was limited due to third party limitations. Now, you are able to scale tests using our own resources. With support for both Kubernetes and Metronome, all you need to do is provide the platform configuration, sit back, and let Predator deploy runners that load your API from your chosen platform. You're probably eager to get your hands dirty, so let's go ahead and install Predator. Important Deployment Tips # Predator is production ready and fully tested and can be deployed in all of the following platforms listed. Please follow these guidelines when deploying Predator: Deploy Predator using only tagged releases and not latest . Predator-Runner docker image version ( zooz/predator-runner:$TAGGED_VERSION ) must match the Predator's major.minor version running in order to be fully compatible with all features. Patched versions don't have to match, but it is recommended to use latest patch version. For example, Predator version 1.6 ( zooz/predator:1.6 ) should run Predator-Runner version 1.6 as well ( zooz/predator-runner:1.6 ). Kubernetes # Install Predator from the Helm Hub DC/OS # Predator can be installed through DC/OS Universe within the cluster. For examples and more info check Universe Catalog Docker # Predator runs in a docker and when installing it using Docker , Predator creates and runs other dockers which actually create the load (predator-runner). In order to avoid DIND, Predator will start its runners as siblings using the docker daemon socket. Command: Without persisted storage: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator:1.5 With persisted storage: docker run -d -e SQLITE_STORAGE=db/predator -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/predator:/usr/db zooz/predator:1.5 Explanations: When starting Predator in a docker we will mount the docker socket to the container. This will allow Predator to start the siblings dockers (Predator-Runner). This is done as so: -v /var/run/docker.sock:/var/run/docker.sock When running tests, the Predator-Runners will have to reach the main Predator to report test results through their internal API. Therefore, Predator has to know it's own accessible address and pass it to the Predator-Runners for them to access Predator's API. This is done by setting the enviornment variable INTERNAL_ADDRESS as so: -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 where $MACHINE_IP is the local ip address of your machine (not localhost, but actual ip address - it is your local network address). In unix or mac this command should give you the ip address and set it to the MACHINE_IP variable: export MACHINE_IP=$(ipconfig getifaddr en0 || ifconfig eth0|grep 'inet addr:'|cut -d':' -f2|awk '{ print $1}') The environment variable JOB_PLATFORM is set to DOCKER so that Predator deploys the Predator-Runners as dockers on the machine it is running. After successfully starting the Predator docker image, access Predator by typing the following URL in your browser: http://{$MACHINE_IP}/ui If you don't see test reports This usually means that the predator-runner couldn't reach the main Predator's API and update it about test progress; docker logs $(docker ps -a -f name=predator -n 1 --format='{{ .ID }}') Will help to understand what went wrong.","title":"Installation"},{"location":"installation.html#installing-predator","text":"Before Predator, running two or more tests simultaneously was limited due to third party limitations. Now, you are able to scale tests using our own resources. With support for both Kubernetes and Metronome, all you need to do is provide the platform configuration, sit back, and let Predator deploy runners that load your API from your chosen platform. You're probably eager to get your hands dirty, so let's go ahead and install Predator.","title":"Installing Predator"},{"location":"installation.html#important-deployment-tips","text":"Predator is production ready and fully tested and can be deployed in all of the following platforms listed. Please follow these guidelines when deploying Predator: Deploy Predator using only tagged releases and not latest . Predator-Runner docker image version ( zooz/predator-runner:$TAGGED_VERSION ) must match the Predator's major.minor version running in order to be fully compatible with all features. Patched versions don't have to match, but it is recommended to use latest patch version. For example, Predator version 1.6 ( zooz/predator:1.6 ) should run Predator-Runner version 1.6 as well ( zooz/predator-runner:1.6 ).","title":"Important Deployment Tips"},{"location":"installation.html#kubernetes","text":"Install Predator from the Helm Hub","title":"Kubernetes"},{"location":"installation.html#dcos","text":"Predator can be installed through DC/OS Universe within the cluster. For examples and more info check Universe Catalog","title":"DC/OS"},{"location":"installation.html#docker","text":"Predator runs in a docker and when installing it using Docker , Predator creates and runs other dockers which actually create the load (predator-runner). In order to avoid DIND, Predator will start its runners as siblings using the docker daemon socket. Command: Without persisted storage: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator:1.5 With persisted storage: docker run -d -e SQLITE_STORAGE=db/predator -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/predator:/usr/db zooz/predator:1.5 Explanations: When starting Predator in a docker we will mount the docker socket to the container. This will allow Predator to start the siblings dockers (Predator-Runner). This is done as so: -v /var/run/docker.sock:/var/run/docker.sock When running tests, the Predator-Runners will have to reach the main Predator to report test results through their internal API. Therefore, Predator has to know it's own accessible address and pass it to the Predator-Runners for them to access Predator's API. This is done by setting the enviornment variable INTERNAL_ADDRESS as so: -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 where $MACHINE_IP is the local ip address of your machine (not localhost, but actual ip address - it is your local network address). In unix or mac this command should give you the ip address and set it to the MACHINE_IP variable: export MACHINE_IP=$(ipconfig getifaddr en0 || ifconfig eth0|grep 'inet addr:'|cut -d':' -f2|awk '{ print $1}') The environment variable JOB_PLATFORM is set to DOCKER so that Predator deploys the Predator-Runners as dockers on the machine it is running. After successfully starting the Predator docker image, access Predator by typing the following URL in your browser: http://{$MACHINE_IP}/ui If you don't see test reports This usually means that the predator-runner couldn't reach the main Predator's API and update it about test progress; docker logs $(docker ps -a -f name=predator -n 1 --format='{{ .ID }}') Will help to understand what went wrong.","title":"Docker"},{"location":"myfirsttest.html","text":"Adding your First Test # In this section, we will walk you through the steps of creating a simple test in Predator. It will allow you to familiarize yourself with some basic concepts, before moving onto some more advanced features later on. Following along with the examples To make it easy for you to follow along with the steps below, we created a demo Predator docker image. This image allows you to invoke our fictitious petstore API, used in the examples that follow. You can retrieve and run the docker image with the following command (just make sure to replace {MACHINE_IP} with the IP address of your own machine): docker run -d -p 3000:3000 --name predator-petstore zooz/predator-builds:petstore Adding a new test is easy. From the Predator web interface, choose Tests > View Tests . Then click Create Test and complete all fields on the form, like so: Test Scenarios # Now proceed to add one or more test scenarios . A scenario is a sequence of HTTP requests aimed to test the performance of a single piece of functionality. For instance, a scenario could test the performance of multiple users requesting information about a specific pet simultaneously. Another scenario could be ordering a pet from the pet store. To add a new scenario, create a new test or edit an existing one. Then click the scenario button and do the following: Set a scenario weight . Allows for the probability of a scenario being picked by a new virtual user to be \"weighed\" relative to other scenarios. If not specified, each scenario is equally likely to be picked. Click the Steps button to add scenario steps . This allows you to add the . Here's a sample scenario that fetches the inventory from the pet store: Pre-scenario Requests # Sometimes a prerequisite must be fulfilled before your scenarios can actually work. For example, pets must already have been created before you can fetch them from the inventory. This is where a pre-scenario request comes into play. A pre-scenario request is an HTTP request that Predator executes before running the scenarios in your test. To add a pre-scenario request, create a new test or edit an existing one. Then click the before button and add all request specifications. HTTP Request Properties # When adding scenario steps or pre-scenario requests, you will need to define the properties of the HTTP request that will be invoked: While most of the properties are self-explanatory, the following items may require some additional explanation: gzip : This will compress the request body. forever : Indicates whether the connection to the endpoint will be kept alive. Captures : Allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. See Data Reuse with Variables . Running the Test # With all scenarios in place, let's go ahead and run the test. Predator executes tests through so-called jobs . To create a job, choose Tests > View tests . Then click RUN for the test you want to execute and complete all fields in the Create new job dialog. When done, click SUBMIT . Depending on your configuration, the job will either execute immediately or at scheduled intervals. Note To see the intervals at which your jobs will run, see Scheduled Tasks . The following table explains the job parameters you can configure: Setting Description Notes Free text describing the job. Arrival rate The number of times per second that the test scenarios will run. Duration (minutes) The time during which the test will run. In minutes. Ramp to Used in combination with the arrival rate and duration values. Increases the arrival rate linearly to the value specified, within the specified duration. Parallelism The number of runners that will be allocated to executing the test. The arrival rate , duration and Max virtual users will be split between the specified number of runners. Max virtual users The maximum number of virtual users executing the scenario requests. This places a threshold on the number of requests that can exist simultaneously. Environment Free text describing the environment against which the test is executed. Cron expression A cron expression for scheduling the test to run periodically at a specified date/time. Run immediately Determines if the test will be executed immediately when the job is saved. Emails An email addresses to which Predator will send a message when the test execution is completed. Webhooks A URLs to which an event will be sent when the test execution is completed. The event body will include detailed information about the test, such as the number of scenarios that were executed and the number of requests that were invoked. Debug If debug is turned on, the predator runner will log every request response. Viewing the Test Report # Your curiosity is probably reaching an all-time high right now, as Predator is working hard to push your API to its limits. So how's your API performing under all that pressure? Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports > Last Reports to view a detailed report of the test that was executed. For more information about the data shown in the test reports, see Test Reports .","title":"My First Test"},{"location":"myfirsttest.html#adding-your-first-test","text":"In this section, we will walk you through the steps of creating a simple test in Predator. It will allow you to familiarize yourself with some basic concepts, before moving onto some more advanced features later on. Following along with the examples To make it easy for you to follow along with the steps below, we created a demo Predator docker image. This image allows you to invoke our fictitious petstore API, used in the examples that follow. You can retrieve and run the docker image with the following command (just make sure to replace {MACHINE_IP} with the IP address of your own machine): docker run -d -p 3000:3000 --name predator-petstore zooz/predator-builds:petstore Adding a new test is easy. From the Predator web interface, choose Tests > View Tests . Then click Create Test and complete all fields on the form, like so:","title":"Adding your First Test"},{"location":"myfirsttest.html#test-scenarios","text":"Now proceed to add one or more test scenarios . A scenario is a sequence of HTTP requests aimed to test the performance of a single piece of functionality. For instance, a scenario could test the performance of multiple users requesting information about a specific pet simultaneously. Another scenario could be ordering a pet from the pet store. To add a new scenario, create a new test or edit an existing one. Then click the scenario button and do the following: Set a scenario weight . Allows for the probability of a scenario being picked by a new virtual user to be \"weighed\" relative to other scenarios. If not specified, each scenario is equally likely to be picked. Click the Steps button to add scenario steps . This allows you to add the . Here's a sample scenario that fetches the inventory from the pet store:","title":"Test Scenarios"},{"location":"myfirsttest.html#pre-scenario-requests","text":"Sometimes a prerequisite must be fulfilled before your scenarios can actually work. For example, pets must already have been created before you can fetch them from the inventory. This is where a pre-scenario request comes into play. A pre-scenario request is an HTTP request that Predator executes before running the scenarios in your test. To add a pre-scenario request, create a new test or edit an existing one. Then click the before button and add all request specifications.","title":"Pre-scenario Requests"},{"location":"myfirsttest.html#http-request-properties","text":"When adding scenario steps or pre-scenario requests, you will need to define the properties of the HTTP request that will be invoked: While most of the properties are self-explanatory, the following items may require some additional explanation: gzip : This will compress the request body. forever : Indicates whether the connection to the endpoint will be kept alive. Captures : Allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. See Data Reuse with Variables .","title":"HTTP Request Properties"},{"location":"myfirsttest.html#running-the-test","text":"With all scenarios in place, let's go ahead and run the test. Predator executes tests through so-called jobs . To create a job, choose Tests > View tests . Then click RUN for the test you want to execute and complete all fields in the Create new job dialog. When done, click SUBMIT . Depending on your configuration, the job will either execute immediately or at scheduled intervals. Note To see the intervals at which your jobs will run, see Scheduled Tasks . The following table explains the job parameters you can configure: Setting Description Notes Free text describing the job. Arrival rate The number of times per second that the test scenarios will run. Duration (minutes) The time during which the test will run. In minutes. Ramp to Used in combination with the arrival rate and duration values. Increases the arrival rate linearly to the value specified, within the specified duration. Parallelism The number of runners that will be allocated to executing the test. The arrival rate , duration and Max virtual users will be split between the specified number of runners. Max virtual users The maximum number of virtual users executing the scenario requests. This places a threshold on the number of requests that can exist simultaneously. Environment Free text describing the environment against which the test is executed. Cron expression A cron expression for scheduling the test to run periodically at a specified date/time. Run immediately Determines if the test will be executed immediately when the job is saved. Emails An email addresses to which Predator will send a message when the test execution is completed. Webhooks A URLs to which an event will be sent when the test execution is completed. The event body will include detailed information about the test, such as the number of scenarios that were executed and the number of requests that were invoked. Debug If debug is turned on, the predator runner will log every request response.","title":"Running the Test"},{"location":"myfirsttest.html#viewing-the-test-report","text":"Your curiosity is probably reaching an all-time high right now, as Predator is working hard to push your API to its limits. So how's your API performing under all that pressure? Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports > Last Reports to view a detailed report of the test that was executed. For more information about the data shown in the test reports, see Test Reports .","title":"Viewing the Test Report"},{"location":"performance.html","text":"Predator Runner Performance # While Predator is designed to support an unlimited load, it's important to understand how this can be achieved. Predator is the mothership and it's responsible to spin up multiple load generators, aka multiple Predator-Runners. The ability to spin up an endless amount of Predator-Runners (as long as you have the required resources) allows Predator to create unlimited distributed load. Predator-Runner # Predator-Runner is a Node.js project which runs a custom version of artillery at its core. While Node.js is a good fit for the use-case of Predator-Runner as its main responsibility is to fire HTTP requests (Non-blocking I/O), it's also limited to 1 core usage (when cluster/child modules are not involved, in which case here they are not). It's also important to note that when a Node.js process reaches near 100% CPU it's starting to report inaccurate results about latency because the event loop is extremely slow. The Three Variables # Predator allows users to choose the arrival rate and not the concurrent users, meaning that users of Predator can aim to exact RPS they want to test. With that being said, each predator-runner's throughput depends on 3 params: arrival rate , max virtual users and the latency of the fired request. Max virtual users means that if there are more than (X) concurrent requests, new requests will be dropped in order to not overkill the CPU (can be seen as avoidedScenarios statistic in the metrics and reports). We suggest set 'Max virtual users' to 250. Arrival rate in its best, each Predator-Runner can do 1200 RPS. But this is correlated to the latency of the requests and 'Max virtual users' param. For example: 1. The average latency is 200ms, this means each virtual users can do 5 RPS, so 250 virtual users can do 1250 RPS. 2. The average latency is 500ms, this means each virtual user can do 2 RPS, so 250 virtual users can do 500 RPS. Conclusions # To summarize the above, if average request latency is below 200ms, 'Predator-runner' will be able to hit 1200 RPS. If it's latency is higher, it will do less. Not all CPU's are equal Not all CPU's are equal, the recommendation in this page written after thorough testing with an AWS EC2 m4.2xlarge machine.","title":"Runner's Performance"},{"location":"performance.html#predator-runner-performance","text":"While Predator is designed to support an unlimited load, it's important to understand how this can be achieved. Predator is the mothership and it's responsible to spin up multiple load generators, aka multiple Predator-Runners. The ability to spin up an endless amount of Predator-Runners (as long as you have the required resources) allows Predator to create unlimited distributed load.","title":"Predator Runner Performance"},{"location":"performance.html#predator-runner","text":"Predator-Runner is a Node.js project which runs a custom version of artillery at its core. While Node.js is a good fit for the use-case of Predator-Runner as its main responsibility is to fire HTTP requests (Non-blocking I/O), it's also limited to 1 core usage (when cluster/child modules are not involved, in which case here they are not). It's also important to note that when a Node.js process reaches near 100% CPU it's starting to report inaccurate results about latency because the event loop is extremely slow.","title":"Predator-Runner"},{"location":"performance.html#the-three-variables","text":"Predator allows users to choose the arrival rate and not the concurrent users, meaning that users of Predator can aim to exact RPS they want to test. With that being said, each predator-runner's throughput depends on 3 params: arrival rate , max virtual users and the latency of the fired request. Max virtual users means that if there are more than (X) concurrent requests, new requests will be dropped in order to not overkill the CPU (can be seen as avoidedScenarios statistic in the metrics and reports). We suggest set 'Max virtual users' to 250. Arrival rate in its best, each Predator-Runner can do 1200 RPS. But this is correlated to the latency of the requests and 'Max virtual users' param. For example: 1. The average latency is 200ms, this means each virtual users can do 5 RPS, so 250 virtual users can do 1250 RPS. 2. The average latency is 500ms, this means each virtual user can do 2 RPS, so 250 virtual users can do 500 RPS.","title":"The Three Variables"},{"location":"performance.html#conclusions","text":"To summarize the above, if average request latency is below 200ms, 'Predator-runner' will be able to hit 1200 RPS. If it's latency is higher, it will do less. Not all CPU's are equal Not all CPU's are equal, the recommendation in this page written after thorough testing with an AWS EC2 m4.2xlarge machine.","title":"Conclusions"},{"location":"plugins.html","text":"Plugins # Metrics and test reports are an essential part of Predator. For this, Predator can be integrated to send test metrics to either Prometheus or Influx, and to send final test results straight to your email upon test completion. SMTP Server - Email Notifier # Set up a connection to your SMTP server to receive email notifactions. Please refer to configuration section to see required variables. Prometheus # Set up a connection to your Prometheus Push-Gateway to receive test run metrics. Please refer to configuration section to see required variables. For reference, Predator uses the following plugin to export Prometheus metrics. Default bucket sizes of [0.01, 0.05, 0.010, 0.50, 0.100, 0.200, 0.300, 0.400, 0.500, 1, 2, 5, 10, 30, 60, 120] are configured but can be changed through Predator's Prometheus metrics configuration. Some of the metrics pushed by Predator-Runner to the Prometheus are: request_with_phases_duration_seconds : Histogram that represents the duration of phases in the request in seconds, and consists of the following label names: ['path', 'status_code', 'phase', 'request_name'] path : path of the request status_code : response status code phase : phase of the request, can be one of the following wait : duration of socket initialization dns : duration of DNS lookup tcp : duration of TCP connection response : duration of HTTP server response total : duration entire HTTP round-trip request_name : name of the request (if not defined then the name = method + path) A dashboard for Grafana can be found here InfluxDB # Set up a connection to your InfluxDB to receive test run metrics. Please refer to configuration section to see required variables. For reference, Predator uses the following plugin to export InfluxDB metrics.","title":"Plugins"},{"location":"plugins.html#plugins","text":"Metrics and test reports are an essential part of Predator. For this, Predator can be integrated to send test metrics to either Prometheus or Influx, and to send final test results straight to your email upon test completion.","title":"Plugins"},{"location":"plugins.html#smtp-server-email-notifier","text":"Set up a connection to your SMTP server to receive email notifactions. Please refer to configuration section to see required variables.","title":"SMTP Server - Email Notifier"},{"location":"plugins.html#prometheus","text":"Set up a connection to your Prometheus Push-Gateway to receive test run metrics. Please refer to configuration section to see required variables. For reference, Predator uses the following plugin to export Prometheus metrics. Default bucket sizes of [0.01, 0.05, 0.010, 0.50, 0.100, 0.200, 0.300, 0.400, 0.500, 1, 2, 5, 10, 30, 60, 120] are configured but can be changed through Predator's Prometheus metrics configuration. Some of the metrics pushed by Predator-Runner to the Prometheus are: request_with_phases_duration_seconds : Histogram that represents the duration of phases in the request in seconds, and consists of the following label names: ['path', 'status_code', 'phase', 'request_name'] path : path of the request status_code : response status code phase : phase of the request, can be one of the following wait : duration of socket initialization dns : duration of DNS lookup tcp : duration of TCP connection response : duration of HTTP server response total : duration entire HTTP round-trip request_name : name of the request (if not defined then the name = method + path) A dashboard for Grafana can be found here","title":"Prometheus"},{"location":"plugins.html#influxdb","text":"Set up a connection to your InfluxDB to receive test run metrics. Please refer to configuration section to see required variables. For reference, Predator uses the following plugin to export InfluxDB metrics.","title":"InfluxDB"},{"location":"processors.html","text":"Supported from version zooz/predator:1.2.0 Use processors to inject custom JavaScript into the test flows. This allows the test flow to be as flexible as possible by creating application resources along the way that are needed for future requests or flow processing. Tip Processors allow you to create anything that's possible in JavaScript and integrate it seemlessly into your test flow. Use-cases # There are many use-cases for injecting custom JavaScript into your test flow. Some include: Prerequistes for a request Create resources needed for an API request, such as an authentication token, that shouldn't be calculated into the report's response times for the overall test flow. Example Random data generation Generate random data to send in your request bodies Example Response logging Log errors if exist Example Creating a processor # Creating a processor can be easily done through the Predator UI. An initial template is provided but it is not required to follow it, we recommend using appropriate function names in order to make it easier to import them into your tests. Extra documentation In order to read more documentation regarding how processors are used in artillery tests, please refer to the artillery docs . Template # module.exports = { beforeScenario, afterScenario, beforeRequest, afterResponse }; function beforeScenario(context, ee, next) { return next(); // MUST be called for the scenario to continue } function afterScenario(context, ee, next) { return next(); // MUST be called for the scenario to continue } function beforeRequest(requestParams, context, ee, next) { return next(); // MUST be called for the scenario to continue } function afterResponse(requestParams, response, context, ee, next) { return next(); // MUST be called for the scenario to continue } Description # Since Predator uses Artillery's built-in processor functionality, there are 4 instances where the processor's Javascript code can be executed. beforeScenario(context, ee, next) # These functions will be executed once before a scenario. All before scenarios functions must adhere to the (context, ee, next) parameters in their function signature. afterScenario(context, ee, next) # These functions will be executed once after a scenario. All after scenarios functions must adhere to the (context, ee, next) parameters in their function signature. beforeRequest(requestParams, context, ee, next) # These functions will be executed once before a specific request. All before requests functions must adhere to the (requestParams, context, ee, next) parameters in their function signature. afterResponse(requestParams, response, context, ee, next) # These functions will be executed once after a specific request. All after response functions must adhere to the (requestParams, response, context, ee, next) parameters in their function signature. Use case examples: # 1. Prerequistes for a request # const uuid = require('uuid/v4'); module.exports = { createAuthToken }; function createAuthToken(context, ee, next) { context.vars.token = uuid(); return next(); // MUST be called for the scenario to continue } 2. Random data generation # module.exports = { generateRandomName }; function generateRandomName(context, ee, next) { context.vars.name = 'random_name_' + Date.now(); return next(); // MUST be called for the scenario to continue } 3. Response logging # module.exports = { logErrorByStatusCode }; function logErrorByStatusCode(requestParams, response, context, ee, next) { if (response.statusCode >= 300) { console.log(`**************** fail with status code: ${JSON.stringify(response.statusCode)}`); console.log(`**************** response body: ${JSON.stringify(response.body)}`); console.log(`**************** response headers is: ${JSON.stringify(response.headers)}`); console.log(`**************** request body: ${JSON.stringify(requestParams.body)}`); } return next(); // MUST be called for the scenario to continue } Using in a test # In order to load a processor into a test, choose a processor from the list Processor in the create/edit test screen. The exported functions from the processor will dynamically load into the Before Scenario , After Scenario , Before Request , After Request dropdown menus. Example # In this example, we loaded a handleErrors processor with the following JavaScript: const uuid = require('uuid/v4'); module.exports = { throwErrorOnError, logResponseOnError, beforeRequest, afterResponse, afterScenario, beforeScenario }; function beforeRequest(requestParams, context, ee, next) { console.log('before request') return next(); // MUST be called for the scenario to continue } function afterResponse(requestParams, response, context, ee, next) { console.log('after response') return next(); // MUST be called for the scenario to continue } function afterScenario(context, ee, next) { console.log('after scenario') return next(); // MUST be called for the scenario to continue } function beforeScenario(context, ee, next) { console.log('before scenario') return next(); // MUST be called for the scenario to continue } function throwErrorOnError(requestParams, response, context, ee, next) { console.log('**************** fail with status code: ' + JSON.stringify(response.statusCode)); console.log('**************** host is: ' + JSON.stringify(response.request.uri.host)); console.log('**************** path is: ' + JSON.stringify(response.request.uri.pathname)); console.log('**************** response headers is: ' + JSON.stringify(response.headers)); throw new Error('Stopping test'); } function logResponseOnError(requestParams, response, context, ee, next) { if(response.statusCode !== 200 || esponse.statusCode !== 201 || esponse.statusCode !== 204){ console.log('**************** fail with status code: ' + JSON.stringify(response.statusCode)); console.log('**************** host is: ' + JSON.stringify(response.request.uri.host)); console.log('**************** path is: ' + JSON.stringify(response.request.uri.pathname)); console.log('**************** response headers is: ' + JSON.stringify(response.headers)); } return next(); // MUST be called for the scenario to continue } Scenario # beforeScenario and afterScenario functions will be executed before and after the scenario ends. Step (request) # logResponseOnError will be executed after each request.","title":"Processor Creation"},{"location":"processors.html#use-cases","text":"There are many use-cases for injecting custom JavaScript into your test flow. Some include: Prerequistes for a request Create resources needed for an API request, such as an authentication token, that shouldn't be calculated into the report's response times for the overall test flow. Example Random data generation Generate random data to send in your request bodies Example Response logging Log errors if exist Example","title":"Use-cases"},{"location":"processors.html#creating-a-processor","text":"Creating a processor can be easily done through the Predator UI. An initial template is provided but it is not required to follow it, we recommend using appropriate function names in order to make it easier to import them into your tests. Extra documentation In order to read more documentation regarding how processors are used in artillery tests, please refer to the artillery docs .","title":"Creating a processor"},{"location":"processors.html#template","text":"module.exports = { beforeScenario, afterScenario, beforeRequest, afterResponse }; function beforeScenario(context, ee, next) { return next(); // MUST be called for the scenario to continue } function afterScenario(context, ee, next) { return next(); // MUST be called for the scenario to continue } function beforeRequest(requestParams, context, ee, next) { return next(); // MUST be called for the scenario to continue } function afterResponse(requestParams, response, context, ee, next) { return next(); // MUST be called for the scenario to continue }","title":"Template"},{"location":"processors.html#description","text":"Since Predator uses Artillery's built-in processor functionality, there are 4 instances where the processor's Javascript code can be executed.","title":"Description"},{"location":"processors.html#beforescenariocontext-ee-next","text":"These functions will be executed once before a scenario. All before scenarios functions must adhere to the (context, ee, next) parameters in their function signature.","title":"beforeScenario(context, ee, next)"},{"location":"processors.html#afterscenariocontext-ee-next","text":"These functions will be executed once after a scenario. All after scenarios functions must adhere to the (context, ee, next) parameters in their function signature.","title":"afterScenario(context, ee, next)"},{"location":"processors.html#beforerequestrequestparams-context-ee-next","text":"These functions will be executed once before a specific request. All before requests functions must adhere to the (requestParams, context, ee, next) parameters in their function signature.","title":"beforeRequest(requestParams, context, ee, next)"},{"location":"processors.html#afterresponserequestparams-response-context-ee-next","text":"These functions will be executed once after a specific request. All after response functions must adhere to the (requestParams, response, context, ee, next) parameters in their function signature.","title":"afterResponse(requestParams, response, context, ee, next)"},{"location":"processors.html#use-case-examples","text":"","title":"Use case examples:"},{"location":"processors.html#1-prerequistes-for-a-request","text":"const uuid = require('uuid/v4'); module.exports = { createAuthToken }; function createAuthToken(context, ee, next) { context.vars.token = uuid(); return next(); // MUST be called for the scenario to continue }","title":"1. Prerequistes for a request"},{"location":"processors.html#2-random-data-generation","text":"module.exports = { generateRandomName }; function generateRandomName(context, ee, next) { context.vars.name = 'random_name_' + Date.now(); return next(); // MUST be called for the scenario to continue }","title":"2. Random data generation"},{"location":"processors.html#3-response-logging","text":"module.exports = { logErrorByStatusCode }; function logErrorByStatusCode(requestParams, response, context, ee, next) { if (response.statusCode >= 300) { console.log(`**************** fail with status code: ${JSON.stringify(response.statusCode)}`); console.log(`**************** response body: ${JSON.stringify(response.body)}`); console.log(`**************** response headers is: ${JSON.stringify(response.headers)}`); console.log(`**************** request body: ${JSON.stringify(requestParams.body)}`); } return next(); // MUST be called for the scenario to continue }","title":"3. Response logging"},{"location":"processors.html#using-in-a-test","text":"In order to load a processor into a test, choose a processor from the list Processor in the create/edit test screen. The exported functions from the processor will dynamically load into the Before Scenario , After Scenario , Before Request , After Request dropdown menus.","title":"Using in a test"},{"location":"processors.html#example","text":"In this example, we loaded a handleErrors processor with the following JavaScript: const uuid = require('uuid/v4'); module.exports = { throwErrorOnError, logResponseOnError, beforeRequest, afterResponse, afterScenario, beforeScenario }; function beforeRequest(requestParams, context, ee, next) { console.log('before request') return next(); // MUST be called for the scenario to continue } function afterResponse(requestParams, response, context, ee, next) { console.log('after response') return next(); // MUST be called for the scenario to continue } function afterScenario(context, ee, next) { console.log('after scenario') return next(); // MUST be called for the scenario to continue } function beforeScenario(context, ee, next) { console.log('before scenario') return next(); // MUST be called for the scenario to continue } function throwErrorOnError(requestParams, response, context, ee, next) { console.log('**************** fail with status code: ' + JSON.stringify(response.statusCode)); console.log('**************** host is: ' + JSON.stringify(response.request.uri.host)); console.log('**************** path is: ' + JSON.stringify(response.request.uri.pathname)); console.log('**************** response headers is: ' + JSON.stringify(response.headers)); throw new Error('Stopping test'); } function logResponseOnError(requestParams, response, context, ee, next) { if(response.statusCode !== 200 || esponse.statusCode !== 201 || esponse.statusCode !== 204){ console.log('**************** fail with status code: ' + JSON.stringify(response.statusCode)); console.log('**************** host is: ' + JSON.stringify(response.request.uri.host)); console.log('**************** path is: ' + JSON.stringify(response.request.uri.pathname)); console.log('**************** response headers is: ' + JSON.stringify(response.headers)); } return next(); // MUST be called for the scenario to continue }","title":"Example"},{"location":"processors.html#scenario","text":"beforeScenario and afterScenario functions will be executed before and after the scenario ends.","title":"Scenario"},{"location":"processors.html#step-request","text":"logResponseOnError will be executed after each request.","title":"Step (request)"},{"location":"schedulesandreports.html","text":"Scheduled Jobs # Predator creates scheduled jobs for all tests that are configured to run at predefined intervals by specifying a cron expression . To see the scheduled jobs that are registered, choose Scheduled Jobs Enable/Disable # Disable or enable back a scheduled job easily through the UI to have more control over the scheduled jobs. Test Reports # Predator's test reports give in-depth performance metrics in real-time, while aggregating test results to an overall RPS and success rate. Comparing Reports # Supported from version zooz/predator:1.3.0 Easily compare two or more test results in one predator report dashboard. This is available under Last Reports page and after selecting the desired tests, clicking on Compare Reports will display the test results side by side on the same graphs. Favorite Reports # Supported from version zooz/predator:1.5.0 Want to save a report and then find it easily after? In the report you would like to favorite, click on the star to add it to a favorites list. Under the reports page you can then filter only the favorite reports and find them in one-click.","title":"Schedules and Reports"},{"location":"schedulesandreports.html#scheduled-jobs","text":"Predator creates scheduled jobs for all tests that are configured to run at predefined intervals by specifying a cron expression . To see the scheduled jobs that are registered, choose Scheduled Jobs","title":"Scheduled Jobs"},{"location":"schedulesandreports.html#enabledisable","text":"Disable or enable back a scheduled job easily through the UI to have more control over the scheduled jobs.","title":"Enable/Disable"},{"location":"schedulesandreports.html#test-reports","text":"Predator's test reports give in-depth performance metrics in real-time, while aggregating test results to an overall RPS and success rate.","title":"Test Reports"},{"location":"schedulesandreports.html#comparing-reports","text":"Supported from version zooz/predator:1.3.0 Easily compare two or more test results in one predator report dashboard. This is available under Last Reports page and after selecting the desired tests, clicking on Compare Reports will display the test results side by side on the same graphs.","title":"Comparing Reports"},{"location":"schedulesandreports.html#favorite-reports","text":"Supported from version zooz/predator:1.5.0 Want to save a report and then find it easily after? In the report you would like to favorite, click on the star to add it to a favorites list. Under the reports page you can then filter only the favorite reports and find them in one-click.","title":"Favorite Reports"},{"location":"streaming.html","text":"Streaming # Supported from version zooz/predator:1.6.0 Produce informative resources to streaming platforms that will allow you to create consumers and handle the data as you see fit. The resources published have a generic schema with all of the test, job, and reports information associated with the event that was triggered. For more specific details about the attributes published refer here . Setting Up # Currently, only Kafka is supported. We will appreciate contributions for more streaming platform integrations :) Kafka # For the full configuration needed please refer to: Kafka configuration manual Resources published # { metadata: metadata object expressing predator and runner versions event: event type resource: { test_id: test id report_id: report id job_id: job id test_name: test name description: test description revision_id: test revision id artillery_test: full artillery test object job_type: job type max_virtual_users: job max virutal users arrival_count: job arrival count arrival_rate: job arrival rate ramp_to: job ramp to parallelism: job parallelism start_time: job start time end_time: job end time notes: job notes duration: job duration status: report status intermediates: array of intermediate results split into 30 second buckets aggregate: object of report aggregated results } } Excluding attributes of resource published # If you would like to exclude some properties published under the resource content, configure the streaming_excluded_attributes in the configuration.","title":"Streaming Platforms"},{"location":"streaming.html#streaming","text":"Supported from version zooz/predator:1.6.0 Produce informative resources to streaming platforms that will allow you to create consumers and handle the data as you see fit. The resources published have a generic schema with all of the test, job, and reports information associated with the event that was triggered. For more specific details about the attributes published refer here .","title":"Streaming"},{"location":"streaming.html#setting-up","text":"Currently, only Kafka is supported. We will appreciate contributions for more streaming platform integrations :)","title":"Setting Up"},{"location":"streaming.html#kafka","text":"For the full configuration needed please refer to: Kafka configuration manual","title":"Kafka"},{"location":"streaming.html#resources-published","text":"{ metadata: metadata object expressing predator and runner versions event: event type resource: { test_id: test id report_id: report id job_id: job id test_name: test name description: test description revision_id: test revision id artillery_test: full artillery test object job_type: job type max_virtual_users: job max virutal users arrival_count: job arrival count arrival_rate: job arrival rate ramp_to: job ramp to parallelism: job parallelism start_time: job start time end_time: job end time notes: job notes duration: job duration status: report status intermediates: array of intermediate results split into 30 second buckets aggregate: object of report aggregated results } }","title":"Resources published"},{"location":"streaming.html#excluding-attributes-of-resource-published","text":"If you would like to exclude some properties published under the resource content, configure the streaming_excluded_attributes in the configuration.","title":"Excluding attributes of resource published"},{"location":"tests.html","text":"What you should already know We assume you have already familiarized yourself with the basic concepts used in Predator and that you successfully created your first test. If not, we strongly recommend you first complete the steps in the My First Test topic before proceeding with the instructions below. Data Reuse with Variables # The Captures field of the HTTP request properties allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. Use JSONPath syntax to extract the data of your choice. In the following example we extract the id field and store it in a petId variable: You can then use petId in a request by placing it between double brackets, like so: {{petId}} Here's an example of using {{petId}} in the request path: Pick a random element from an array # When capturing responses data, you may want to randomly select an item from your dataset to continue with your scenario. This is achieved by using the * operator, which selects all the matching items. The engine will then randomly pick a single item for you and store it in the defined variable. Usage example : on [ { \"id\" : \"45697038-37ae-4149-9f5e-ed7a4a20e014\" , \"data\" : \"132456789\" }, { \"id\" : \"f02392e8-1b11-401f-a902-8a9a1ae5c47a\" , \"data\" : \"abcdefghi\" } ] will randomly store 45697038-...-ed7a4a20e014 or f02392e8-...-8a9a1ae5c47a in {{dataId}} for further usage in the scenario. CSV Files # Supported from version zooz/predator:1.4.0 Predator supports uploading CSV files and using them as data set for the test. Each new virtual user that starts a scenario will pick a row from the data set. Usage Example: Save a csv file with headers and delimited by ',' amount, currency 10, USD 30, GBP 50, EUR When creating a test, click on add 'Add csv' and drag and drop the file into the box All headers available in the csv are ready to use as variables Built-in Functions # Predator supports some generic functions out of the box and they can be used to generate random data. $randomNumber(min, max) will generate a random number within the provided range. $randomString(length) will generate a random string with the specified length. $uuid() will generate v4 UUID. $dateNow() will generate a number of ms since epoch. Currently these functions all return values as a string . An issue is open regarding this: #178 Usage example: { \"id\" : \"{{ $uuid() }}\" , \"name\" : \"{{ $randomString(6) }}\" , \"age\" : \"{{ $randomNumber(0,15) }}\" , \"created\" : \"{{ $dateNow() }}\" } Request Reuse with DSL Definitions # This is the moment where Predator shows its teeth and unleashes its true power. Writing a performance test that checks specific parts of your API end-to-end can be a huge hassle, but now it is effortless. By creating DSL definitions using Predator's Domain Specific Language (DSL), request templates are generated which you can then reuse in the same test and in other tests under the same DSL type, reducing replication. Let's dive right in and get going with our first DSL definition. Predator API This functionality is only available through the Predator API . Creating a DSL Definition # Before you can use a DSL definition, you must create it first. You do so by invoking the Create DSL Definition request. Here's an example request body for creating a DSL definition of a GET request. Notice how we use the {{petId}} in the url endpoint (we will create this variable in the example of a POST request DSL definition): { \"name\" : \"get-pet\" , \"request\" : { \"get\" : { \"url\" : \"http://127.0.0.1:3000/pets/{{petId}}\" } } } The request body for creating a DSL definition of a POST request is a bit more elaborate, since it requires that you pass in the entire body that makes up the POST request. The following example shows how to do this. Notice how we add a capture array, in which we define the petId variable for storing the pet ID. We can then reuse it in another request (like in the example of a GET request above). { \"name\" : \"create-pet\" , \"request\" : { \"post\" : { \"url\" : \"http://127.0.0.1:3000/pets\" , \"json\" : { \"name\" : \"mickey\" , \"species\" : \"dog\" }, \"capture\" : [ { \"json\" : \"$.id\" , \"as\" : \"petId\" } ] } } } Creating a Test that Uses the DSL # Tests that use a DSL definition can only be created using the Create Test API request. The Create Test API request body must include all components that make up a test, including pre-scenario requests and scenarios. However, instead of defining the entire HTTP request in each scenario step (as you would through the Predator UI), you can now reference the HTTP request through its DSL definition. You do so, using the action property (in the steps array). Here's an example: { \"name\" : \"Pet store\" , \"description\" : \"DSL\" , \"type\" : \"dsl\" , // Make sure t he t ype is se t t o DSL \"before\" : { \"steps\" : [ { \"action\" : \"petstore.create-pet\" } ] }, \"scenarios\" : [ { \"scenario_name\" : \"Only get pet\" , \"steps\" : [ { \"action\" : \"petstore.get-pet\" } ] } ] } There are two additional items to note: The type must always be set to dsl . The action value uses the following syntax: {dsl_group}.{dsl_name} , in which the dsl_group is the name used in the path of the Create DSL Definition API request. If you login to the Predator UI after creating the test, you will notice that the test has been added with a type of dsl . You can now run the test as you would any other. Creating a Test with Custom Logic in Javascript (Released in version 1.2.0) # Tests can use custom Javascript functions. To use custom Javascript functions in your tests please refer to the processors documentation . Debugging Test Requests/Responses # It is possible to view all of the requests and responses that the predator-runner sends and receives while running load on the API. This is very useful when your test does not behave the way you expected it to and need the request/response to further investigate. The predator-runner will log this data to the log files which can be downloaded. For the test to run in debug mode: 1. UI # Turn on the Debug flag when running a test 2. API # Add \"debug\": \"*\" to the body of the POST /jobs request","title":"Advanced Test Setup"},{"location":"tests.html#data-reuse-with-variables","text":"The Captures field of the HTTP request properties allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. Use JSONPath syntax to extract the data of your choice. In the following example we extract the id field and store it in a petId variable: You can then use petId in a request by placing it between double brackets, like so: {{petId}} Here's an example of using {{petId}} in the request path:","title":"Data Reuse with Variables"},{"location":"tests.html#pick-a-random-element-from-an-array","text":"When capturing responses data, you may want to randomly select an item from your dataset to continue with your scenario. This is achieved by using the * operator, which selects all the matching items. The engine will then randomly pick a single item for you and store it in the defined variable. Usage example : on [ { \"id\" : \"45697038-37ae-4149-9f5e-ed7a4a20e014\" , \"data\" : \"132456789\" }, { \"id\" : \"f02392e8-1b11-401f-a902-8a9a1ae5c47a\" , \"data\" : \"abcdefghi\" } ] will randomly store 45697038-...-ed7a4a20e014 or f02392e8-...-8a9a1ae5c47a in {{dataId}} for further usage in the scenario.","title":"Pick a random element from an array"},{"location":"tests.html#csv-files","text":"Supported from version zooz/predator:1.4.0 Predator supports uploading CSV files and using them as data set for the test. Each new virtual user that starts a scenario will pick a row from the data set. Usage Example: Save a csv file with headers and delimited by ',' amount, currency 10, USD 30, GBP 50, EUR When creating a test, click on add 'Add csv' and drag and drop the file into the box All headers available in the csv are ready to use as variables","title":"CSV Files"},{"location":"tests.html#built-in-functions","text":"Predator supports some generic functions out of the box and they can be used to generate random data. $randomNumber(min, max) will generate a random number within the provided range. $randomString(length) will generate a random string with the specified length. $uuid() will generate v4 UUID. $dateNow() will generate a number of ms since epoch. Currently these functions all return values as a string . An issue is open regarding this: #178 Usage example: { \"id\" : \"{{ $uuid() }}\" , \"name\" : \"{{ $randomString(6) }}\" , \"age\" : \"{{ $randomNumber(0,15) }}\" , \"created\" : \"{{ $dateNow() }}\" }","title":"Built-in Functions"},{"location":"tests.html#request-reuse-with-dsl-definitions","text":"This is the moment where Predator shows its teeth and unleashes its true power. Writing a performance test that checks specific parts of your API end-to-end can be a huge hassle, but now it is effortless. By creating DSL definitions using Predator's Domain Specific Language (DSL), request templates are generated which you can then reuse in the same test and in other tests under the same DSL type, reducing replication. Let's dive right in and get going with our first DSL definition. Predator API This functionality is only available through the Predator API .","title":"Request Reuse with DSL Definitions"},{"location":"tests.html#creating-a-dsl-definition","text":"Before you can use a DSL definition, you must create it first. You do so by invoking the Create DSL Definition request. Here's an example request body for creating a DSL definition of a GET request. Notice how we use the {{petId}} in the url endpoint (we will create this variable in the example of a POST request DSL definition): { \"name\" : \"get-pet\" , \"request\" : { \"get\" : { \"url\" : \"http://127.0.0.1:3000/pets/{{petId}}\" } } } The request body for creating a DSL definition of a POST request is a bit more elaborate, since it requires that you pass in the entire body that makes up the POST request. The following example shows how to do this. Notice how we add a capture array, in which we define the petId variable for storing the pet ID. We can then reuse it in another request (like in the example of a GET request above). { \"name\" : \"create-pet\" , \"request\" : { \"post\" : { \"url\" : \"http://127.0.0.1:3000/pets\" , \"json\" : { \"name\" : \"mickey\" , \"species\" : \"dog\" }, \"capture\" : [ { \"json\" : \"$.id\" , \"as\" : \"petId\" } ] } } }","title":"Creating a DSL Definition"},{"location":"tests.html#creating-a-test-that-uses-the-dsl","text":"Tests that use a DSL definition can only be created using the Create Test API request. The Create Test API request body must include all components that make up a test, including pre-scenario requests and scenarios. However, instead of defining the entire HTTP request in each scenario step (as you would through the Predator UI), you can now reference the HTTP request through its DSL definition. You do so, using the action property (in the steps array). Here's an example: { \"name\" : \"Pet store\" , \"description\" : \"DSL\" , \"type\" : \"dsl\" , // Make sure t he t ype is se t t o DSL \"before\" : { \"steps\" : [ { \"action\" : \"petstore.create-pet\" } ] }, \"scenarios\" : [ { \"scenario_name\" : \"Only get pet\" , \"steps\" : [ { \"action\" : \"petstore.get-pet\" } ] } ] } There are two additional items to note: The type must always be set to dsl . The action value uses the following syntax: {dsl_group}.{dsl_name} , in which the dsl_group is the name used in the path of the Create DSL Definition API request. If you login to the Predator UI after creating the test, you will notice that the test has been added with a type of dsl . You can now run the test as you would any other.","title":"Creating a Test that Uses the DSL"},{"location":"tests.html#creating-a-test-with-custom-logic-in-javascript-released-in-version-120","text":"Tests can use custom Javascript functions. To use custom Javascript functions in your tests please refer to the processors documentation .","title":"Creating a Test with Custom Logic in Javascript (Released in version 1.2.0)"},{"location":"tests.html#debugging-test-requestsresponses","text":"It is possible to view all of the requests and responses that the predator-runner sends and receives while running load on the API. This is very useful when your test does not behave the way you expected it to and need the request/response to further investigate. The predator-runner will log this data to the log files which can be downloaded. For the test to run in debug mode:","title":"Debugging Test Requests/Responses"},{"location":"tests.html#1-ui","text":"Turn on the Debug flag when running a test","title":"1. UI"},{"location":"tests.html#2-api","text":"Add \"debug\": \"*\" to the body of the POST /jobs request","title":"2. API"},{"location":"webhooks.html","text":"Webhooks # Supported from version zooz/predator:1.5.0 Webhooks are events that notify you on test progress. Webhooks are supported in Slack or JSON format for an easy server to server integration. You can define a global webhook which will be enabled for system-wide tests or an ad hoc webhook which will be optional on a specific test run. Webhook Events # The following test run events are supported when configuring a webhook: Started : Sent when a test starts. In progress : Sent after a test run receives its first statistics. API Failure : Sent if there are 5xx status codes. Aborted : Sent when a test is aborted. Failed : Sent when a test fails to run. Finished : Sent when a test finishes successfully. Benchmark Passed : Sent when a test finishes successfully and receives an equal or higher score than the allowed threshold. Benchmark Failed : Sent when a test finishes successfully and receives a lower score than the allowed threshold. Setting Up # Webhooks can be set up both in the UI and in the API. For further info please see our API Reference . Global Webhook # Global webhooks are invoked on all test runs. Ad hoc Webhook # Ad hoc webhooks can be paired with a specific test run (either by API/UI). Webhook Formats # Slack # Webhooks can be sent in as a Slack message to any Slack channel with a proper Slack webhook URL. JSON # For server to server integration, webhooks can also be sent as an HTTP POST request to a configured webhook URL with relevant data in JSON content-type regarding the test's progress and results. TEAMS # Webhooks can be sent in as a Microsoft Teams message to any Teams channel with a proper incoming webhook URL. DISCORD # Webhooks can be sent in as a Discord message to any Discord channel with a proper incoming webhook URL. Example # A global webhook created in Slack format that will invoke a message to the configured Slack channel's URL on every test run that's in the following phases: started in_progress aborted failed finished","title":"Webhooks"},{"location":"webhooks.html#webhooks","text":"Supported from version zooz/predator:1.5.0 Webhooks are events that notify you on test progress. Webhooks are supported in Slack or JSON format for an easy server to server integration. You can define a global webhook which will be enabled for system-wide tests or an ad hoc webhook which will be optional on a specific test run.","title":"Webhooks"},{"location":"webhooks.html#webhook-events","text":"The following test run events are supported when configuring a webhook: Started : Sent when a test starts. In progress : Sent after a test run receives its first statistics. API Failure : Sent if there are 5xx status codes. Aborted : Sent when a test is aborted. Failed : Sent when a test fails to run. Finished : Sent when a test finishes successfully. Benchmark Passed : Sent when a test finishes successfully and receives an equal or higher score than the allowed threshold. Benchmark Failed : Sent when a test finishes successfully and receives a lower score than the allowed threshold.","title":"Webhook Events"},{"location":"webhooks.html#setting-up","text":"Webhooks can be set up both in the UI and in the API. For further info please see our API Reference .","title":"Setting Up"},{"location":"webhooks.html#global-webhook","text":"Global webhooks are invoked on all test runs.","title":"Global Webhook"},{"location":"webhooks.html#ad-hoc-webhook","text":"Ad hoc webhooks can be paired with a specific test run (either by API/UI).","title":"Ad hoc Webhook"},{"location":"webhooks.html#webhook-formats","text":"","title":"Webhook Formats"},{"location":"webhooks.html#slack","text":"Webhooks can be sent in as a Slack message to any Slack channel with a proper Slack webhook URL.","title":"Slack"},{"location":"webhooks.html#json","text":"For server to server integration, webhooks can also be sent as an HTTP POST request to a configured webhook URL with relevant data in JSON content-type regarding the test's progress and results.","title":"JSON"},{"location":"webhooks.html#teams","text":"Webhooks can be sent in as a Microsoft Teams message to any Teams channel with a proper incoming webhook URL.","title":"TEAMS"},{"location":"webhooks.html#discord","text":"Webhooks can be sent in as a Discord message to any Discord channel with a proper incoming webhook URL.","title":"DISCORD"},{"location":"webhooks.html#example","text":"A global webhook created in Slack format that will invoke a message to the configured Slack channel's URL on every test run that's in the following phases: started in_progress aborted failed finished","title":"Example"}]}