{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Introducing Predator # An open-source high performance testing framework Requests per second, request latency, and overall system performance and reliability are some fundamental concepts that need to be taken into account when designing a high capacity API. With CI/CD becoming a common deployment methodology, deployments to production are a constant occurrence. So how can you ensure that the core capabilities you initially designed your system to have remain intact? The answer is: use Predator! Predator is a performance framework that can be configured to automatically load your API with scheduled tests that provide in-depth performance metrics. While sipping on your (morning) coffee, you can simply check the test summaries to make sure your system is performing as expected. By comparing test results to results from previous test runs, you can easily identify system-wide regressions. Predator\u2019s integration with custom dashboards, test metrics, and service logs make pinpointing performance bugs painless,especially when you check up on the system on a daily basis. The following features guided us throughout our development, and they shape the core ideology of Predator.","title":"About"},{"location":"index.html#introducing-predator","text":"An open-source high performance testing framework Requests per second, request latency, and overall system performance and reliability are some fundamental concepts that need to be taken into account when designing a high capacity API. With CI/CD becoming a common deployment methodology, deployments to production are a constant occurrence. So how can you ensure that the core capabilities you initially designed your system to have remain intact? The answer is: use Predator! Predator is a performance framework that can be configured to automatically load your API with scheduled tests that provide in-depth performance metrics. While sipping on your (morning) coffee, you can simply check the test summaries to make sure your system is performing as expected. By comparing test results to results from previous test runs, you can easily identify system-wide regressions. Predator\u2019s integration with custom dashboards, test metrics, and service logs make pinpointing performance bugs painless,especially when you check up on the system on a daily basis. The following features guided us throughout our development, and they shape the core ideology of Predator.","title":"Introducing Predator"},{"location":"contributing.html","text":"Contribute to Predator! Contributor Guidelines # Contributing Documentation Changes # Contributing Code Changes #","title":"Contributing"},{"location":"contributing.html#contributor-guidelines","text":"","title":"Contributor Guidelines"},{"location":"contributing.html#contributing-documentation-changes","text":"","title":"Contributing Documentation Changes"},{"location":"contributing.html#contributing-code-changes","text":"","title":"Contributing Code Changes"},{"location":"installation.html","text":"Installing Predator # Before Predator, running two or more tests simultaneously was limited due to third party limitations. Now, you are able to scale tests using our own resources. With support for both Kubernetes and Metronome, all you need to do is provide the platform configuration, sit back, and let Predator deploy runners that load your API from your chosen platform. You're probably eager to get your hands dirty, so let's go ahead and install Predator. Kubernetes # Bacon ipsum dolor amet cupim chicken pork ribeye. Short loin ball tip jowl beef. Ball tip strip steak jowl tail shoulder doner chicken salami beef ribs pork short ribs swine ham hock landjaeger biltong. Ribeye cow filet mignon landjaeger. pork --save DC/OS # Jowl pancetta meatloaf short ribs buffalo. Leberkas meatball alcatra chuck, capicola buffalo spare ribs shankle sirloin tenderloin landjaeger salami meatloaf biltong. Flank pancetta meatball turkey chuck tenderloin bresaola biltong prosciutto andouille. Turducken jowl ball tip short loin. Docker # Predator runs using Docker. In order to run Predator locally run the following command: runPredatorLocal.sh You can also manually run the following command: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator where $MACHINE_IP is the local ip address of your machine. After successfully mounting the Predator docker image, access Predator by typing the following URL in your browser: http://{ipaddress}/predator","title":"Installation"},{"location":"installation.html#installing-predator","text":"Before Predator, running two or more tests simultaneously was limited due to third party limitations. Now, you are able to scale tests using our own resources. With support for both Kubernetes and Metronome, all you need to do is provide the platform configuration, sit back, and let Predator deploy runners that load your API from your chosen platform. You're probably eager to get your hands dirty, so let's go ahead and install Predator.","title":"Installing Predator"},{"location":"installation.html#kubernetes","text":"Bacon ipsum dolor amet cupim chicken pork ribeye. Short loin ball tip jowl beef. Ball tip strip steak jowl tail shoulder doner chicken salami beef ribs pork short ribs swine ham hock landjaeger biltong. Ribeye cow filet mignon landjaeger. pork --save","title":"Kubernetes"},{"location":"installation.html#dcos","text":"Jowl pancetta meatloaf short ribs buffalo. Leberkas meatball alcatra chuck, capicola buffalo spare ribs shankle sirloin tenderloin landjaeger salami meatloaf biltong. Flank pancetta meatball turkey chuck tenderloin bresaola biltong prosciutto andouille. Turducken jowl ball tip short loin.","title":"DC/OS"},{"location":"installation.html#docker","text":"Predator runs using Docker. In order to run Predator locally run the following command: runPredatorLocal.sh You can also manually run the following command: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator where $MACHINE_IP is the local ip address of your machine. After successfully mounting the Predator docker image, access Predator by typing the following URL in your browser: http://{ipaddress}/predator","title":"Docker"},{"location":"myfirsttest.html","text":"Adding your First Test # In this section, we will walk you through the steps of creating a simple test in Predator. It will allow you to familiarize yourself with some basic concepts, before moving onto some more advanced features later on. Following along with the examples To make it easy for you to follow along with the steps below, we created a demo Predator docker image. This image allows you to invoke our fictitious petstore API, used in the examples that follow. You can retrieve and run the docker image with the following command (just make sure to replace {MACHINE_IP} with the IP address of your own machine): docker run -d -p 3000:3000 --name predator-petstore zooz/predator-builds:petstore GET: Adding a new test is easy. From the Predator web interface, choose Tests View Tests . Then click Create Test and complete all fields on the form, like so: Test Scenarios # Now proceed to add one or more test scenarios . A scenario is a sequence of HTTP requests aimed to test the performance of a single piece of functionality. For instance, a scenario could test the performance of multiple users requesting information about a specific pet simultaneously. Another scenario could be ordering a pet from the pet store. To add a new scenario, create a new test or edit an existing one. Then click the scenario button and do the following: Set a scenario weight . Allows for the probability of a scenario being picked by a new virtual user to be \"weighed\" relative to other scenarios. If not specified, each scenario is equally likely to be picked. Click the Steps button to add scenario steps . This allows you to add the . Here's a sample scenario that fetches the inventory from the pet store: Pre-scenario Requests # Sometimes a prerequisite must be fulfilled before your scenarios can actually work. For example, pets must already have been created before you can fetch them from the inventory. This is where a pre-scenario request comes into play. A pre-scenario request is an HTTP request that Predator executes before running the scenarios in your test. To add a pre-scenario request, create a new test or edit an existing one. Then click the before button and add all request specifications. HTTP Request Properties # When adding scenario steps or pre-scenario requests, you will need to define the properties of the HTTP request that will be invoked: While most of the properties are self-explanatory, the following items may require some additional explanation: gzip : This will compress the request body. forever : Indicates whether the connection to the endpoint will be kept alive. Captures : Allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. See Data Reuse with Variables . Running the Test # With all scenarios in place, let's go ahead and run the test. Predator executes tests through so-called jobs . To create a job, choose Tests View tests . Then click RUN for the test you want to execute and complete all fields in the Create new job dialog. When done, click SUBMIT . Depending on your configuration, the job will either execute immediately or at scheduled intervals. Note To see the intervals at which your jobs will run, see Scheduled Tasks . The following table explains the job parameters you can configure: Setting Description Notes Free text describing the job. Arrival rate The number of times per second that the test scenarios will run. Duration (seconds) The time during which the test will run. In seconds. Ramp to Used in combination with the arrival rate and duration values. Increases the arrival rate linearly to the value specified, within the specified duration. Parallelism The number of runners that will be allocated to executing the test. The arrival rate , duration and Max virtual users will be split between the specified number of runners. Max virtual users The maximum number of virtual users executing the scenario requests. This places a threshold on the number of requests that can exist simultaneously. Environment Free text describing the environment against which the test is executed. Cron expression A cron expression for scheduling the test to run periodically at a specified date/time. Run immediately Determines if the test will be executed immediately when the job is saved. New Email An email address to which Predator will send a message when the test execution is completed. New Webhook A URL to which an event will be sent when the test execution is completed. The event body will include detailed information about the test, such as the number of scenarios that were executed and the number of requests that were invoked. Viewing the Test Report # Your curiosity is probably reaching an all-time high right now, as Predator is working hard to push your API to its limits. So how's your API performing under all that pressure? Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view a detailed report of the test that was executed. For more information about the data shown in the test reports, see Test Reports .","title":"My First Test"},{"location":"myfirsttest.html#adding-your-first-test","text":"In this section, we will walk you through the steps of creating a simple test in Predator. It will allow you to familiarize yourself with some basic concepts, before moving onto some more advanced features later on. Following along with the examples To make it easy for you to follow along with the steps below, we created a demo Predator docker image. This image allows you to invoke our fictitious petstore API, used in the examples that follow. You can retrieve and run the docker image with the following command (just make sure to replace {MACHINE_IP} with the IP address of your own machine): docker run -d -p 3000:3000 --name predator-petstore zooz/predator-builds:petstore GET: Adding a new test is easy. From the Predator web interface, choose Tests View Tests . Then click Create Test and complete all fields on the form, like so:","title":"Adding your First Test"},{"location":"myfirsttest.html#test-scenarios","text":"Now proceed to add one or more test scenarios . A scenario is a sequence of HTTP requests aimed to test the performance of a single piece of functionality. For instance, a scenario could test the performance of multiple users requesting information about a specific pet simultaneously. Another scenario could be ordering a pet from the pet store. To add a new scenario, create a new test or edit an existing one. Then click the scenario button and do the following: Set a scenario weight . Allows for the probability of a scenario being picked by a new virtual user to be \"weighed\" relative to other scenarios. If not specified, each scenario is equally likely to be picked. Click the Steps button to add scenario steps . This allows you to add the . Here's a sample scenario that fetches the inventory from the pet store:","title":"Test Scenarios"},{"location":"myfirsttest.html#pre-scenario-requests","text":"Sometimes a prerequisite must be fulfilled before your scenarios can actually work. For example, pets must already have been created before you can fetch them from the inventory. This is where a pre-scenario request comes into play. A pre-scenario request is an HTTP request that Predator executes before running the scenarios in your test. To add a pre-scenario request, create a new test or edit an existing one. Then click the before button and add all request specifications.","title":"Pre-scenario Requests"},{"location":"myfirsttest.html#http-request-properties","text":"When adding scenario steps or pre-scenario requests, you will need to define the properties of the HTTP request that will be invoked: While most of the properties are self-explanatory, the following items may require some additional explanation: gzip : This will compress the request body. forever : Indicates whether the connection to the endpoint will be kept alive. Captures : Allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. See Data Reuse with Variables .","title":"HTTP Request Properties"},{"location":"myfirsttest.html#running-the-test","text":"With all scenarios in place, let's go ahead and run the test. Predator executes tests through so-called jobs . To create a job, choose Tests View tests . Then click RUN for the test you want to execute and complete all fields in the Create new job dialog. When done, click SUBMIT . Depending on your configuration, the job will either execute immediately or at scheduled intervals. Note To see the intervals at which your jobs will run, see Scheduled Tasks . The following table explains the job parameters you can configure: Setting Description Notes Free text describing the job. Arrival rate The number of times per second that the test scenarios will run. Duration (seconds) The time during which the test will run. In seconds. Ramp to Used in combination with the arrival rate and duration values. Increases the arrival rate linearly to the value specified, within the specified duration. Parallelism The number of runners that will be allocated to executing the test. The arrival rate , duration and Max virtual users will be split between the specified number of runners. Max virtual users The maximum number of virtual users executing the scenario requests. This places a threshold on the number of requests that can exist simultaneously. Environment Free text describing the environment against which the test is executed. Cron expression A cron expression for scheduling the test to run periodically at a specified date/time. Run immediately Determines if the test will be executed immediately when the job is saved. New Email An email address to which Predator will send a message when the test execution is completed. New Webhook A URL to which an event will be sent when the test execution is completed. The event body will include detailed information about the test, such as the number of scenarios that were executed and the number of requests that were invoked.","title":"Running the Test"},{"location":"myfirsttest.html#viewing-the-test-report","text":"Your curiosity is probably reaching an all-time high right now, as Predator is working hard to push your API to its limits. So how's your API performing under all that pressure? Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view a detailed report of the test that was executed. For more information about the data shown in the test reports, see Test Reports .","title":"Viewing the Test Report"},{"location":"plugins.html","text":"","title":"Plugins"},{"location":"schedulesandreports.html","text":"Scheduled Tasks # Predator creates scheduled tasks for all tests that you scheduled to run at predefined intervals. To see the scheduled tasks that are registered in the system, choose Tasks Scheduled Tasks Test Reports # Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view detailed reports of the tests that was executed.","title":"Schedules and Reports"},{"location":"schedulesandreports.html#scheduled-tasks","text":"Predator creates scheduled tasks for all tests that you scheduled to run at predefined intervals. To see the scheduled tasks that are registered in the system, choose Tasks Scheduled Tasks","title":"Scheduled Tasks"},{"location":"schedulesandreports.html#test-reports","text":"Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view detailed reports of the tests that was executed.","title":"Test Reports"},{"location":"tests.html","text":"What you should already know We assume you have already familiarized yourself with the basic concepts used in Predator and that you successfully created your first test. If not, we strongly recommend you first complete the steps in the My First Test topic before proceeding with the instructions below. Data Reuse with Variables # The Captures field of the HTTP request properties allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. Use JSONPath syntax to extract the data of your choice. In the following example we extract the id field and store it in a petId variable: You can then use petId in a request by placing it between double brackets, like so: {{petId}} Here's an example of using {{petId}} in the request path: Request Reuse with DSL Definitions # This is the moment where Predator shows its teeth and unleashes its true power. Writing a performance test that checks specific parts of your API end-to-end can be a huge hassle, but now it is effortless. By creating DSL definitions using Predator's Domain Specific Language (DSL), request templates are generated which you can then reuse in the same test and in other tests under the same DSL type, reducing replication. Let's dive right in and get going with our first DSL definition. Predator API This functionality is only available through the Predator API . Creating a DSL Definition # Before you can use a DSL definition, you must create it first. You do so by invoking the Create DSL request . Here's an example request body for creating a DSL definition of a GET request. Notice how we use the {{petId}} in the url endpoint (we will create this variable in the example of a POST request DSL definition): { \"name\": \"get-pet\", \"request\": { \"get\": { \"url\": \"http://127.0.0.1:3000/pets/{{petId}}\" } } } The request body for creating a DSL definition of a POST request is a bit more elaborate, since it requires that you pass in the entire body that makes up the POST request. The following example shows how to do this. Notice how we add a capture array, in which we define the petId variable for storing the pet ID. We can then reuse it in another request (like in the example of a GET request above). { \"name\": \"create-pet\", \"request\": { \"post\": { \"url\": \"http://127.0.0.1:3000/pets\", \"json\": { \"name\": \"mickey\", \"species\": \"dog\" }, \"capture\": [ { \"json\": \"$.id\", \"as\": \"petId\" } ] } } } Creating a Test that Uses the DSL # Tests that use a DSL definition can only be created using the Create DSL Test API request. The Create DSL Test API request body must include all components that make up a test, including pre-scenario requests and scenarios. However, instead of defining the entire HTTP request in each scenario step (as you would through the Predator UI), you can now reference the HTTP request through its DSL definition. You do so, using the action property (in the steps array). Here's an example: { \"name\": \"Pet store\", \"description\": \"DSL\", \"type\": \"dsl\", // Make sure the type is set to DSL \"before\": { \"steps\": [ { \"action\": \"petstore.create-pet\" } ] }, \"scenarios\": [ { \"scenario_name\": \"Only get pet\", \"steps\": [ { \"action\": \"petstore.get-pet\" } ] } ] } There are two additional items to note: The type must always be set to dsl . The action value uses the following syntax: {dsl_group_name}.{dsl_name} , in which the dsl_group_name is the name used in the path of the Create DSL Defintion API request. If you login to the Predator UI after creating the test, you will notice that the test has been added with a type of dsl . You can now run the test as you would any other.","title":"Advanced Test Setup"},{"location":"tests.html#data-reuse-with-variables","text":"The Captures field of the HTTP request properties allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. Use JSONPath syntax to extract the data of your choice. In the following example we extract the id field and store it in a petId variable: You can then use petId in a request by placing it between double brackets, like so: {{petId}} Here's an example of using {{petId}} in the request path:","title":"Data Reuse with Variables"},{"location":"tests.html#request-reuse-with-dsl-definitions","text":"This is the moment where Predator shows its teeth and unleashes its true power. Writing a performance test that checks specific parts of your API end-to-end can be a huge hassle, but now it is effortless. By creating DSL definitions using Predator's Domain Specific Language (DSL), request templates are generated which you can then reuse in the same test and in other tests under the same DSL type, reducing replication. Let's dive right in and get going with our first DSL definition. Predator API This functionality is only available through the Predator API .","title":"Request Reuse with DSL Definitions"},{"location":"tests.html#creating-a-dsl-definition","text":"Before you can use a DSL definition, you must create it first. You do so by invoking the Create DSL request . Here's an example request body for creating a DSL definition of a GET request. Notice how we use the {{petId}} in the url endpoint (we will create this variable in the example of a POST request DSL definition): { \"name\": \"get-pet\", \"request\": { \"get\": { \"url\": \"http://127.0.0.1:3000/pets/{{petId}}\" } } } The request body for creating a DSL definition of a POST request is a bit more elaborate, since it requires that you pass in the entire body that makes up the POST request. The following example shows how to do this. Notice how we add a capture array, in which we define the petId variable for storing the pet ID. We can then reuse it in another request (like in the example of a GET request above). { \"name\": \"create-pet\", \"request\": { \"post\": { \"url\": \"http://127.0.0.1:3000/pets\", \"json\": { \"name\": \"mickey\", \"species\": \"dog\" }, \"capture\": [ { \"json\": \"$.id\", \"as\": \"petId\" } ] } } }","title":"Creating a DSL Definition"},{"location":"tests.html#creating-a-test-that-uses-the-dsl","text":"Tests that use a DSL definition can only be created using the Create DSL Test API request. The Create DSL Test API request body must include all components that make up a test, including pre-scenario requests and scenarios. However, instead of defining the entire HTTP request in each scenario step (as you would through the Predator UI), you can now reference the HTTP request through its DSL definition. You do so, using the action property (in the steps array). Here's an example: { \"name\": \"Pet store\", \"description\": \"DSL\", \"type\": \"dsl\", // Make sure the type is set to DSL \"before\": { \"steps\": [ { \"action\": \"petstore.create-pet\" } ] }, \"scenarios\": [ { \"scenario_name\": \"Only get pet\", \"steps\": [ { \"action\": \"petstore.get-pet\" } ] } ] } There are two additional items to note: The type must always be set to dsl . The action value uses the following syntax: {dsl_group_name}.{dsl_name} , in which the dsl_group_name is the name used in the path of the Create DSL Defintion API request. If you login to the Predator UI after creating the test, you will notice that the test has been added with a type of dsl . You can now run the test as you would any other.","title":"Creating a Test that Uses the DSL"}]}