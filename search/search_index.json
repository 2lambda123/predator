{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"about.html","text":"Introducing Predator # Distribute open source performance testing platform for APIs. Requests per second, request latency, and overall system performance and reliability are some fundamental concepts that need to be taken into account when designing a high capacity API. With CI/CD becoming a common deployment methodology, deployments to production are a constant occurrence. So how can you ensure that the core capabilities you initially designed your system to have remain intact? The answer is: use Predator! Predator is a performance platform that can be configured to automatically load your API with scheduled tests that provide in-depth performance metrics. While sipping on your (morning) coffee, you can simply check the test summaries to make sure your system is performing as expected. By comparing test results to results from previous test runs, you can easily identify system-wide regressions. Predator\u2019s integration with custom dashboards, test metrics, and service logs make pinpointing performance bugs painless,especially when you check up on the system on a daily basis. Main Features # Distributed load : Predator supports an unlimited number of load generators that produce multiple load runners concurrently. Real time reports : Predator aggregates all concurrent runs into a single beautiful report in real time (latency, rps, status codes and more). Built for the cloud : Predator is built to take advantage of Kubernetes and DC/OS. It's integrated with those platforms and can manage the load generators lifecycles by itself. One click installation : Predator can be installed with just one click in Kubernetes and DC/OS, or on any other machine running Docker. Supports 5 Different databases : Predator provides out-of-the box functionality for persisting data in Cassandra, Postgres, MySQL, MSSQL and SQLITE. Scheduled jobs : Predator can run recurring tests using cron expressions. 3rd partry metrics : Predator comes integrated with Prometheus and Influx. Simply configure it through the predator REST API or using the UI. Rich UI : Predator offers a rich UI along with a powerful REST API. Based on artillery.io : Predator uses artillery as its load engine to fire the requests. The schema for creating tests via the Predator REST API is based on the artillery schema. UI # Under the hood # Feature Comparison #","title":"About"},{"location":"about.html#introducing-predator","text":"Distribute open source performance testing platform for APIs. Requests per second, request latency, and overall system performance and reliability are some fundamental concepts that need to be taken into account when designing a high capacity API. With CI/CD becoming a common deployment methodology, deployments to production are a constant occurrence. So how can you ensure that the core capabilities you initially designed your system to have remain intact? The answer is: use Predator! Predator is a performance platform that can be configured to automatically load your API with scheduled tests that provide in-depth performance metrics. While sipping on your (morning) coffee, you can simply check the test summaries to make sure your system is performing as expected. By comparing test results to results from previous test runs, you can easily identify system-wide regressions. Predator\u2019s integration with custom dashboards, test metrics, and service logs make pinpointing performance bugs painless,especially when you check up on the system on a daily basis.","title":"Introducing Predator"},{"location":"about.html#main-features","text":"Distributed load : Predator supports an unlimited number of load generators that produce multiple load runners concurrently. Real time reports : Predator aggregates all concurrent runs into a single beautiful report in real time (latency, rps, status codes and more). Built for the cloud : Predator is built to take advantage of Kubernetes and DC/OS. It's integrated with those platforms and can manage the load generators lifecycles by itself. One click installation : Predator can be installed with just one click in Kubernetes and DC/OS, or on any other machine running Docker. Supports 5 Different databases : Predator provides out-of-the box functionality for persisting data in Cassandra, Postgres, MySQL, MSSQL and SQLITE. Scheduled jobs : Predator can run recurring tests using cron expressions. 3rd partry metrics : Predator comes integrated with Prometheus and Influx. Simply configure it through the predator REST API or using the UI. Rich UI : Predator offers a rich UI along with a powerful REST API. Based on artillery.io : Predator uses artillery as its load engine to fire the requests. The schema for creating tests via the Predator REST API is based on the artillery schema.","title":"Main Features"},{"location":"about.html#ui","text":"","title":"UI"},{"location":"about.html#under-the-hood","text":"","title":"Under the hood"},{"location":"about.html#feature-comparison","text":"","title":"Feature Comparison"},{"location":"apireference.html","text":"The Predator API gives you advanced control over the tests you create and allows you utilize functionalities not available through the Predator UI. For instance, using the API you can create DSL definitions and reuse them in your requests. Go ahead and dig into our Predator API documentation .","title":"API Reference"},{"location":"configuration.html","text":"Configuration # When running Predator, it is possible to retrieve and update some of the service's configuration during runtime with the /config endpoint, please check the API reference for more details. Below are variables Predator can be configured with. General # Environment Variable Configuration key Description Configurable from UI/API Default value INTERNAL_ADDRESS internal_address The local ip address of your machine \u2713 RUNNER_DOCKER_IMAGE runner_docker_image The predator-runner docker image that will run the test \u2713 zooz/predator-runner:latest RUNNER_CPU runner_cpu Number of CPU use by the each runner \u2713 1 RUNNER_MEMORY runner_memory Max memory to use by each runner \u2713 256 DEFAULT_EMAIL_ADDRESS default_email_address Default email to send final report to, address can be configured \u2713 DEFAULT_WEBHOOK_URL default_webhook_url Default webhook url to send live report statistics to \u2713 ALLOW_INSECURE_TLS allow_insecure_tls If true, don't fail requests on unverified server certificate errors \u2713 false DELAY_RUNNER_MS delay_runner_ms Delay the predator runner from sending http requests (ms) \u2713 INTERVAL_CLEANUP_FINISHED_CONTAINERS_MS interval_cleanup_finished_containers_ms Interval (in ms) to search and delete finished tests containers. Value of 0 means no auto clearing enabled \u2713 0 Database # Environment Variable Description Configurable from UI/API Default value DATABASE_TYPE Database to integrate Predator with [Cassandra, Postgres, MySQL, MSSQL, SQLITE] x SQLITE DATABASE_NAME Database/Keyspace name x DATABASE_ADDRESS Database address x DATABASE_USERNAME Database username x DATABASE_PASSWORD Database password x Additional parameters for the following chosen databases: Cassandra # Environment Variable Configurable from UI/API Default value CASSANDRA_REPLICATION_FACTOR x 1 CASSANDRA_CONSISTENCY x localQuorum CASSANDRA_KEY_SPACE_STRATEGY x SimpleStrategy CASSANDRA_LOCAL_DATA_CENTER x SQLITE # Environment Variable Description Configurable from UI/API Default value SQLITE_STORATE SQLITE file name x predator Deployment # Environment Variable Description Configurable from UI/API Default value JOB_PLATFORM Type of platform using to run predator (METRONOME,KUBERNETES,DOCKER) x DOCKER Kubernetes # Environment Variable Description Configurable from UI/API Default value KUBERNETES_URL Kubernetes API Url x KUBERNETES_TOKEN Kubernetes API Token x KUBERNETES_NAMESPACE Kubernetes Namespace x Metronome # Environment Variable Description Configurable from UI/API Default value METRONOME_URL Metronome API Url x METRONOME_TOKEN Metronome API Token x Docker # Environment Variable Description Configurable from UI/API Default value DOCKER_HOST Docker engine url (host and port number of docker engine) x DOCKER_CERT_PATH Path to CA certificate directory x Metrics # PROCESS.ENV Variable Configuration key Description Configurable from UI/API Default value METRICS_PLUGIN_NAME metrics_plugin_name Metrics integration to use [prometheus,influx] \u2713 Prometheus # Environment Variable Configuration key Description Configurable from UI/API Default value prometheus_metrics.push_gateway_url Url of push gateway \u2713 prometheus_metrics.buckets_sizes Bucket sizes to configure prometheus \u2713 prometheus_metrics.labels Labels will be passed to the push gateway \u2713 Influx # Environment Variable Configuration key Description Configurable from UI/API Default value influx_metrics.host Influx db host \u2713 influx_metrics.username Influx db username \u2713 influx_metrics.password Influx db password \u2713 influx_metrics.database Influx db name \u2713 SMTP Server # Environment Variable Configuration key Description Configurable from UI/API Default value SMTP_FROM smtp_server.from the 'from' email address that will be used to send emails \u2713 SMTP_HOST smtp_server.host SMTP host \u2713 SMTP_PORT smtp_server.port SMTP port number \u2713 SMTP_USERNAME smtp_server.username SMTP username \u2713 SMTP_PASSWORD smtp_server.password SMTP password \u2713 SMTP_TIMEOUT smtp_server.timeout timeout to SMTP server in milliseconds \u2713 200 SMTP_SECURE smtp_server.secure if true the connection will use TLS when connecting to server. Nodemailer SMTP options \u2713 false SMTP_REJECT_UNAUTH_CERTS smtp_server.rejectUnauthCerts should fail or succeed on unauthorized certificate \u2713 false","title":"Configuration"},{"location":"configuration.html#configuration","text":"When running Predator, it is possible to retrieve and update some of the service's configuration during runtime with the /config endpoint, please check the API reference for more details. Below are variables Predator can be configured with.","title":"Configuration"},{"location":"configuration.html#general","text":"Environment Variable Configuration key Description Configurable from UI/API Default value INTERNAL_ADDRESS internal_address The local ip address of your machine \u2713 RUNNER_DOCKER_IMAGE runner_docker_image The predator-runner docker image that will run the test \u2713 zooz/predator-runner:latest RUNNER_CPU runner_cpu Number of CPU use by the each runner \u2713 1 RUNNER_MEMORY runner_memory Max memory to use by each runner \u2713 256 DEFAULT_EMAIL_ADDRESS default_email_address Default email to send final report to, address can be configured \u2713 DEFAULT_WEBHOOK_URL default_webhook_url Default webhook url to send live report statistics to \u2713 ALLOW_INSECURE_TLS allow_insecure_tls If true, don't fail requests on unverified server certificate errors \u2713 false DELAY_RUNNER_MS delay_runner_ms Delay the predator runner from sending http requests (ms) \u2713 INTERVAL_CLEANUP_FINISHED_CONTAINERS_MS interval_cleanup_finished_containers_ms Interval (in ms) to search and delete finished tests containers. Value of 0 means no auto clearing enabled \u2713 0","title":"General"},{"location":"configuration.html#database","text":"Environment Variable Description Configurable from UI/API Default value DATABASE_TYPE Database to integrate Predator with [Cassandra, Postgres, MySQL, MSSQL, SQLITE] x SQLITE DATABASE_NAME Database/Keyspace name x DATABASE_ADDRESS Database address x DATABASE_USERNAME Database username x DATABASE_PASSWORD Database password x Additional parameters for the following chosen databases:","title":"Database"},{"location":"configuration.html#cassandra","text":"Environment Variable Configurable from UI/API Default value CASSANDRA_REPLICATION_FACTOR x 1 CASSANDRA_CONSISTENCY x localQuorum CASSANDRA_KEY_SPACE_STRATEGY x SimpleStrategy CASSANDRA_LOCAL_DATA_CENTER x","title":"Cassandra"},{"location":"configuration.html#sqlite","text":"Environment Variable Description Configurable from UI/API Default value SQLITE_STORATE SQLITE file name x predator","title":"SQLITE"},{"location":"configuration.html#deployment","text":"Environment Variable Description Configurable from UI/API Default value JOB_PLATFORM Type of platform using to run predator (METRONOME,KUBERNETES,DOCKER) x DOCKER","title":"Deployment"},{"location":"configuration.html#kubernetes","text":"Environment Variable Description Configurable from UI/API Default value KUBERNETES_URL Kubernetes API Url x KUBERNETES_TOKEN Kubernetes API Token x KUBERNETES_NAMESPACE Kubernetes Namespace x","title":"Kubernetes"},{"location":"configuration.html#metronome","text":"Environment Variable Description Configurable from UI/API Default value METRONOME_URL Metronome API Url x METRONOME_TOKEN Metronome API Token x","title":"Metronome"},{"location":"configuration.html#docker","text":"Environment Variable Description Configurable from UI/API Default value DOCKER_HOST Docker engine url (host and port number of docker engine) x DOCKER_CERT_PATH Path to CA certificate directory x","title":"Docker"},{"location":"configuration.html#metrics","text":"PROCESS.ENV Variable Configuration key Description Configurable from UI/API Default value METRICS_PLUGIN_NAME metrics_plugin_name Metrics integration to use [prometheus,influx] \u2713","title":"Metrics"},{"location":"configuration.html#prometheus","text":"Environment Variable Configuration key Description Configurable from UI/API Default value prometheus_metrics.push_gateway_url Url of push gateway \u2713 prometheus_metrics.buckets_sizes Bucket sizes to configure prometheus \u2713 prometheus_metrics.labels Labels will be passed to the push gateway \u2713","title":"Prometheus"},{"location":"configuration.html#influx","text":"Environment Variable Configuration key Description Configurable from UI/API Default value influx_metrics.host Influx db host \u2713 influx_metrics.username Influx db username \u2713 influx_metrics.password Influx db password \u2713 influx_metrics.database Influx db name \u2713","title":"Influx"},{"location":"configuration.html#smtp-server","text":"Environment Variable Configuration key Description Configurable from UI/API Default value SMTP_FROM smtp_server.from the 'from' email address that will be used to send emails \u2713 SMTP_HOST smtp_server.host SMTP host \u2713 SMTP_PORT smtp_server.port SMTP port number \u2713 SMTP_USERNAME smtp_server.username SMTP username \u2713 SMTP_PASSWORD smtp_server.password SMTP password \u2713 SMTP_TIMEOUT smtp_server.timeout timeout to SMTP server in milliseconds \u2713 200 SMTP_SECURE smtp_server.secure if true the connection will use TLS when connecting to server. Nodemailer SMTP options \u2713 false SMTP_REJECT_UNAUTH_CERTS smtp_server.rejectUnauthCerts should fail or succeed on unauthorized certificate \u2713 false","title":"SMTP Server"},{"location":"contributing.html","text":"Guidelines # First off, thanks for taking the time to contribute! The following is a set of guidelines for contributing to Predator and its packages, which are hosted in the Zooz Predator project on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request. Submitting a Bug Report # Bugs are tracked as GitHub issues . Provide the following information when submitting a bug: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. Provide specific examples to demonstrate the steps. Include links to files or GitHub projects, or copy pasteable snippets, which you use in those examples. If you're providing snippets in the issue, use Markdown code blocks. Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why. Contributing Code Changes # As an open-source project, we welcome and encourage the community to submit patches directly to the project. In our collaborative open source environment, standards for submitting changes help reduce the chaos that can result from an active development community. So here are some high-level steps we suggest you follow when contributing code changes: Fork the project clone locally. Create an upstream remote and sync your local copy before you branch. Branch for each separate piece of work. Do the work, write good commit messages. Commit messages must adhere to commitlint standards. Push to your origin repository. Create a new PR in GitHub. Respond to any code review feedback. Contributing Documentation Changes # Documentation is mega-important. Predator cannot truly succeed without great documentation. It\u2019s that simple. So please make sure to provide documentation updates for any new features you contributed, including useful example code snippets. Needless to say, as a user of this project you're perfect for helping us improve our docs. So feel free to report documentation bugs or submit documentation changes through a pull request.","title":"Contributing"},{"location":"contributing.html#guidelines","text":"First off, thanks for taking the time to contribute! The following is a set of guidelines for contributing to Predator and its packages, which are hosted in the Zooz Predator project on GitHub. These are mostly guidelines, not rules. Use your best judgment, and feel free to propose changes to this document in a pull request.","title":"Guidelines"},{"location":"contributing.html#submitting-a-bug-report","text":"Bugs are tracked as GitHub issues . Provide the following information when submitting a bug: Use a clear and descriptive title for the issue to identify the problem. Describe the exact steps which reproduce the problem in as many details as possible. Provide specific examples to demonstrate the steps. Include links to files or GitHub projects, or copy pasteable snippets, which you use in those examples. If you're providing snippets in the issue, use Markdown code blocks. Describe the behavior you observed after following the steps and point out what exactly is the problem with that behavior. Explain which behavior you expected to see instead and why.","title":"Submitting a Bug Report"},{"location":"contributing.html#contributing-code-changes","text":"As an open-source project, we welcome and encourage the community to submit patches directly to the project. In our collaborative open source environment, standards for submitting changes help reduce the chaos that can result from an active development community. So here are some high-level steps we suggest you follow when contributing code changes: Fork the project clone locally. Create an upstream remote and sync your local copy before you branch. Branch for each separate piece of work. Do the work, write good commit messages. Commit messages must adhere to commitlint standards. Push to your origin repository. Create a new PR in GitHub. Respond to any code review feedback.","title":"Contributing Code Changes"},{"location":"contributing.html#contributing-documentation-changes","text":"Documentation is mega-important. Predator cannot truly succeed without great documentation. It\u2019s that simple. So please make sure to provide documentation updates for any new features you contributed, including useful example code snippets. Needless to say, as a user of this project you're perfect for helping us improve our docs. So feel free to report documentation bugs or submit documentation changes through a pull request.","title":"Contributing Documentation Changes"},{"location":"faq.html","text":"Frequently Asked Questions # Installation # Predator is running, how do I access the UI? # The UI is accessible at http://$MACHINE_IP/ui where $MACHINE_IP is your local network address you used to install Predator. Tests # I run a test successfully but no report is created for the test # The Predator-Runner docker that is reporting the test results back to Predator isn't able to connect to it, which is why the test runs but no report is generated. When installing Predator in Docker, the following command is used: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 \\ -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator The INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 is what the Predator-Runner uses to communicate with Predator, and $MACHINE_IP needs to be your local network address (an IP address). You can get it by running the command: ifconfig en0 | grep inet | cut -d -f2 For more information regarding correct installation of Predator using Docker visit the Installation section. It is important to note this is an issue and solution only in Docker installations. In Kubernetes and DC/OS installations the INTERNAL_ADDRESS is built in. What is the http engine Predator uses to run the load? # Predator uses Artillery as its HTTP load engine. Therefore, all basic type tests are written in Artillery syntax and all of the features Artillery supports, Predator supports. To read more about Artillery and its features visit their well written documentary: - Artillery Documentation - Artillery Basic Concepts Documentation - Artillery Test Structure Documentation - Artillery HTTP Engine Documentation I want to use Predator's API, where can I find examples for creating advanced/dsl tests? # Here is a postman collection which contains examples for creating tests with custom javascript and dsl definitions. What content-type does Predator support in its HTTP requests? # While the Predator UI currently supports creating tests only with content-type: application/json body, the actual Predator API has no such limit. When creating a test through the API, instead of specifying in the post request a json key, specify body key and change the content-type header to the appropriate content-type being used. For example: { post : { url : /orders , headers : { Content-Type : text/html }, body : Not Json :) } } In this example, the content-type used in the request is text/html and the body sent: \"Not JSON :)\" will be in that format. Configuration # I ran Predator with SQLITE and would like to migrate now to a different database. How do I do this? # Migration between different databases is not possible. In order to run Predator with a different supported database, you must restart Predator with the new configuration . Metrics # Does Predator support exporting metrics to external time series databases? # Yes, Predator has integration with both Prometheus and InfluxDB , and can export metrics by test endpoints and status codes, something that is currently is not available to configure in the Predator UI, but is supported through the API. How can I export metrics to Prometheus? # Prometheus by its nature is a scraper, while the Predator-Runner is a job without a specific API and finite time life. To overcome this: 1. Deploy Prometheus push gateway 2. Configure Predator to push metrics to the push gateway. 3. Configure Prometheus to scrape from the push gateway. Is there any dashboard for Grafana I can use? # This dashboard for example configured to read from Prometheus.","title":"FAQ"},{"location":"faq.html#frequently-asked-questions","text":"","title":"Frequently Asked Questions"},{"location":"faq.html#installation","text":"","title":"Installation"},{"location":"faq.html#predator-is-running-how-do-i-access-the-ui","text":"The UI is accessible at http://$MACHINE_IP/ui where $MACHINE_IP is your local network address you used to install Predator.","title":"Predator is running, how do I access the UI?"},{"location":"faq.html#tests","text":"","title":"Tests"},{"location":"faq.html#i-run-a-test-successfully-but-no-report-is-created-for-the-test","text":"The Predator-Runner docker that is reporting the test results back to Predator isn't able to connect to it, which is why the test runs but no report is generated. When installing Predator in Docker, the following command is used: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 \\ -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator The INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 is what the Predator-Runner uses to communicate with Predator, and $MACHINE_IP needs to be your local network address (an IP address). You can get it by running the command: ifconfig en0 | grep inet | cut -d -f2 For more information regarding correct installation of Predator using Docker visit the Installation section. It is important to note this is an issue and solution only in Docker installations. In Kubernetes and DC/OS installations the INTERNAL_ADDRESS is built in.","title":"I run a test successfully but no report is created for the test"},{"location":"faq.html#what-is-the-http-engine-predator-uses-to-run-the-load","text":"Predator uses Artillery as its HTTP load engine. Therefore, all basic type tests are written in Artillery syntax and all of the features Artillery supports, Predator supports. To read more about Artillery and its features visit their well written documentary: - Artillery Documentation - Artillery Basic Concepts Documentation - Artillery Test Structure Documentation - Artillery HTTP Engine Documentation","title":"What is the http engine Predator uses to run the load?"},{"location":"faq.html#i-want-to-use-predators-api-where-can-i-find-examples-for-creating-advanceddsl-tests","text":"Here is a postman collection which contains examples for creating tests with custom javascript and dsl definitions.","title":"I want to use Predator's API, where can I find examples for creating advanced/dsl tests?"},{"location":"faq.html#what-content-type-does-predator-support-in-its-http-requests","text":"While the Predator UI currently supports creating tests only with content-type: application/json body, the actual Predator API has no such limit. When creating a test through the API, instead of specifying in the post request a json key, specify body key and change the content-type header to the appropriate content-type being used. For example: { post : { url : /orders , headers : { Content-Type : text/html }, body : Not Json :) } } In this example, the content-type used in the request is text/html and the body sent: \"Not JSON :)\" will be in that format.","title":"What content-type does Predator support in its HTTP requests?"},{"location":"faq.html#configuration","text":"","title":"Configuration"},{"location":"faq.html#i-ran-predator-with-sqlite-and-would-like-to-migrate-now-to-a-different-database-how-do-i-do-this","text":"Migration between different databases is not possible. In order to run Predator with a different supported database, you must restart Predator with the new configuration .","title":"I ran Predator with SQLITE and would like to migrate now to a different database. How do I do this?"},{"location":"faq.html#metrics","text":"","title":"Metrics"},{"location":"faq.html#does-predator-support-exporting-metrics-to-external-time-series-databases","text":"Yes, Predator has integration with both Prometheus and InfluxDB , and can export metrics by test endpoints and status codes, something that is currently is not available to configure in the Predator UI, but is supported through the API.","title":"Does Predator support exporting metrics to external time series databases?"},{"location":"faq.html#how-can-i-export-metrics-to-prometheus","text":"Prometheus by its nature is a scraper, while the Predator-Runner is a job without a specific API and finite time life. To overcome this: 1. Deploy Prometheus push gateway 2. Configure Predator to push metrics to the push gateway. 3. Configure Prometheus to scrape from the push gateway.","title":"How can I export metrics to Prometheus?"},{"location":"faq.html#is-there-any-dashboard-for-grafana-i-can-use","text":"This dashboard for example configured to read from Prometheus.","title":"Is there any dashboard for Grafana I can use?"},{"location":"installation.html","text":"Installing Predator # Before Predator, running two or more tests simultaneously was limited due to third party limitations. Now, you are able to scale tests using our own resources. With support for both Kubernetes and Metronome, all you need to do is provide the platform configuration, sit back, and let Predator deploy runners that load your API from your chosen platform. You're probably eager to get your hands dirty, so let's go ahead and install Predator. Kubernetes # Install Predator from the Helm Hub DC/OS # Predator can be installed through DC/OS Universe within the cluster. For examples and more info check Universe Catalog Docker # Predator runs in a docker and when installing it using Docker , Predator creates and runs other dockers which actually create the load (predator-runner). In order to avoid DIND, Predator will start its runners as siblings using the docker daemon socket. Command: Without persisted storage: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator With persisted storage: docker run -d -e SQLITE_STORAGE=db/predator -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/predator:/usr/db zooz/predator Explanations: When starting Predator in a docker we will mount the docker socket to the container. This will allow Predator to start the siblings dockers (Predator-Runner). This is done as so: -v /var/run/docker.sock:/var/run/docker.sock When running tests, the Predator-Runners will have to reach the main Predator to report test results through their internal API. Therefore, Predator has to know it's own accessible address and pass it to the Predator-Runners for them to access Predator's API. This is done by setting the enviornment variable INTERNAL_ADDRESS as so: -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 where $MACHINE_IP is the local ip address of your machine (not localhost, but actual ip address - it is your local network address). In unix or mac this command should give you the ip address and set it to the MACHINE_IP variable: export MACHINE_IP=$(ipconfig getifaddr en0 || ifconfig eth0|grep 'inet addr:'|cut -d':' -f2|awk '{ print $1}') The environment variable JOB_PLATFORM is set to DOCKER so that Predator deploys the Predator-Runners as dockers on the machine it is running. After successfully starting the Predator docker image, access Predator by typing the following URL in your browser: http://{$MACHINE_IP}/ui If you don't see test reports This usually means that the predator-runner couldn't reach the main Predator's API and update it about test progress; docker logs $(docker ps -a -f name=predator -n 1 --format='{{ .ID }}') Will help to understand what went wrong.","title":"Installation"},{"location":"installation.html#installing-predator","text":"Before Predator, running two or more tests simultaneously was limited due to third party limitations. Now, you are able to scale tests using our own resources. With support for both Kubernetes and Metronome, all you need to do is provide the platform configuration, sit back, and let Predator deploy runners that load your API from your chosen platform. You're probably eager to get your hands dirty, so let's go ahead and install Predator.","title":"Installing Predator"},{"location":"installation.html#kubernetes","text":"Install Predator from the Helm Hub","title":"Kubernetes"},{"location":"installation.html#dcos","text":"Predator can be installed through DC/OS Universe within the cluster. For examples and more info check Universe Catalog","title":"DC/OS"},{"location":"installation.html#docker","text":"Predator runs in a docker and when installing it using Docker , Predator creates and runs other dockers which actually create the load (predator-runner). In order to avoid DIND, Predator will start its runners as siblings using the docker daemon socket. Command: Without persisted storage: docker run -d -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock zooz/predator With persisted storage: docker run -d -e SQLITE_STORAGE=db/predator -e JOB_PLATFORM=DOCKER -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 -p 80:80 --name predator -v /var/run/docker.sock:/var/run/docker.sock -v /tmp/predator:/usr/db zooz/predator Explanations: When starting Predator in a docker we will mount the docker socket to the container. This will allow Predator to start the siblings dockers (Predator-Runner). This is done as so: -v /var/run/docker.sock:/var/run/docker.sock When running tests, the Predator-Runners will have to reach the main Predator to report test results through their internal API. Therefore, Predator has to know it's own accessible address and pass it to the Predator-Runners for them to access Predator's API. This is done by setting the enviornment variable INTERNAL_ADDRESS as so: -e INTERNAL_ADDRESS=http://$MACHINE_IP:80/v1 where $MACHINE_IP is the local ip address of your machine (not localhost, but actual ip address - it is your local network address). In unix or mac this command should give you the ip address and set it to the MACHINE_IP variable: export MACHINE_IP=$(ipconfig getifaddr en0 || ifconfig eth0|grep 'inet addr:'|cut -d':' -f2|awk '{ print $1}') The environment variable JOB_PLATFORM is set to DOCKER so that Predator deploys the Predator-Runners as dockers on the machine it is running. After successfully starting the Predator docker image, access Predator by typing the following URL in your browser: http://{$MACHINE_IP}/ui If you don't see test reports This usually means that the predator-runner couldn't reach the main Predator's API and update it about test progress; docker logs $(docker ps -a -f name=predator -n 1 --format='{{ .ID }}') Will help to understand what went wrong.","title":"Docker"},{"location":"myfirsttest.html","text":"Adding your First Test # In this section, we will walk you through the steps of creating a simple test in Predator. It will allow you to familiarize yourself with some basic concepts, before moving onto some more advanced features later on. Following along with the examples To make it easy for you to follow along with the steps below, we created a demo Predator docker image. This image allows you to invoke our fictitious petstore API, used in the examples that follow. You can retrieve and run the docker image with the following command (just make sure to replace {MACHINE_IP} with the IP address of your own machine): docker run -d -p 3000:3000 --name predator-petstore zooz/predator-builds:petstore Adding a new test is easy. From the Predator web interface, choose Tests View Tests . Then click Create Test and complete all fields on the form, like so: Test Scenarios # Now proceed to add one or more test scenarios . A scenario is a sequence of HTTP requests aimed to test the performance of a single piece of functionality. For instance, a scenario could test the performance of multiple users requesting information about a specific pet simultaneously. Another scenario could be ordering a pet from the pet store. To add a new scenario, create a new test or edit an existing one. Then click the scenario button and do the following: Set a scenario weight . Allows for the probability of a scenario being picked by a new virtual user to be \"weighed\" relative to other scenarios. If not specified, each scenario is equally likely to be picked. Click the Steps button to add scenario steps . This allows you to add the . Here's a sample scenario that fetches the inventory from the pet store: Pre-scenario Requests # Sometimes a prerequisite must be fulfilled before your scenarios can actually work. For example, pets must already have been created before you can fetch them from the inventory. This is where a pre-scenario request comes into play. A pre-scenario request is an HTTP request that Predator executes before running the scenarios in your test. To add a pre-scenario request, create a new test or edit an existing one. Then click the before button and add all request specifications. HTTP Request Properties # When adding scenario steps or pre-scenario requests, you will need to define the properties of the HTTP request that will be invoked: While most of the properties are self-explanatory, the following items may require some additional explanation: gzip : This will compress the request body. forever : Indicates whether the connection to the endpoint will be kept alive. Captures : Allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. See Data Reuse with Variables . Running the Test # With all scenarios in place, let's go ahead and run the test. Predator executes tests through so-called jobs . To create a job, choose Tests View tests . Then click RUN for the test you want to execute and complete all fields in the Create new job dialog. When done, click SUBMIT . Depending on your configuration, the job will either execute immediately or at scheduled intervals. Note To see the intervals at which your jobs will run, see Scheduled Tasks . The following table explains the job parameters you can configure: Setting Description Notes Free text describing the job. Arrival rate The number of times per second that the test scenarios will run. Duration (minutes) The time during which the test will run. In minutes. Ramp to Used in combination with the arrival rate and duration values. Increases the arrival rate linearly to the value specified, within the specified duration. Parallelism The number of runners that will be allocated to executing the test. The arrival rate , duration and Max virtual users will be split between the specified number of runners. Max virtual users The maximum number of virtual users executing the scenario requests. This places a threshold on the number of requests that can exist simultaneously. Environment Free text describing the environment against which the test is executed. Cron expression A cron expression for scheduling the test to run periodically at a specified date/time. Run immediately Determines if the test will be executed immediately when the job is saved. Emails An email addresses to which Predator will send a message when the test execution is completed. Webhooks A URLs to which an event will be sent when the test execution is completed. The event body will include detailed information about the test, such as the number of scenarios that were executed and the number of requests that were invoked. Debug If debug is turned on, the predator runner will log every request response. Viewing the Test Report # Your curiosity is probably reaching an all-time high right now, as Predator is working hard to push your API to its limits. So how's your API performing under all that pressure? Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view a detailed report of the test that was executed. For more information about the data shown in the test reports, see Test Reports .","title":"My First Test"},{"location":"myfirsttest.html#adding-your-first-test","text":"In this section, we will walk you through the steps of creating a simple test in Predator. It will allow you to familiarize yourself with some basic concepts, before moving onto some more advanced features later on. Following along with the examples To make it easy for you to follow along with the steps below, we created a demo Predator docker image. This image allows you to invoke our fictitious petstore API, used in the examples that follow. You can retrieve and run the docker image with the following command (just make sure to replace {MACHINE_IP} with the IP address of your own machine): docker run -d -p 3000:3000 --name predator-petstore zooz/predator-builds:petstore Adding a new test is easy. From the Predator web interface, choose Tests View Tests . Then click Create Test and complete all fields on the form, like so:","title":"Adding your First Test"},{"location":"myfirsttest.html#test-scenarios","text":"Now proceed to add one or more test scenarios . A scenario is a sequence of HTTP requests aimed to test the performance of a single piece of functionality. For instance, a scenario could test the performance of multiple users requesting information about a specific pet simultaneously. Another scenario could be ordering a pet from the pet store. To add a new scenario, create a new test or edit an existing one. Then click the scenario button and do the following: Set a scenario weight . Allows for the probability of a scenario being picked by a new virtual user to be \"weighed\" relative to other scenarios. If not specified, each scenario is equally likely to be picked. Click the Steps button to add scenario steps . This allows you to add the . Here's a sample scenario that fetches the inventory from the pet store:","title":"Test Scenarios"},{"location":"myfirsttest.html#pre-scenario-requests","text":"Sometimes a prerequisite must be fulfilled before your scenarios can actually work. For example, pets must already have been created before you can fetch them from the inventory. This is where a pre-scenario request comes into play. A pre-scenario request is an HTTP request that Predator executes before running the scenarios in your test. To add a pre-scenario request, create a new test or edit an existing one. Then click the before button and add all request specifications.","title":"Pre-scenario Requests"},{"location":"myfirsttest.html#http-request-properties","text":"When adding scenario steps or pre-scenario requests, you will need to define the properties of the HTTP request that will be invoked: While most of the properties are self-explanatory, the following items may require some additional explanation: gzip : This will compress the request body. forever : Indicates whether the connection to the endpoint will be kept alive. Captures : Allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. See Data Reuse with Variables .","title":"HTTP Request Properties"},{"location":"myfirsttest.html#running-the-test","text":"With all scenarios in place, let's go ahead and run the test. Predator executes tests through so-called jobs . To create a job, choose Tests View tests . Then click RUN for the test you want to execute and complete all fields in the Create new job dialog. When done, click SUBMIT . Depending on your configuration, the job will either execute immediately or at scheduled intervals. Note To see the intervals at which your jobs will run, see Scheduled Tasks . The following table explains the job parameters you can configure: Setting Description Notes Free text describing the job. Arrival rate The number of times per second that the test scenarios will run. Duration (minutes) The time during which the test will run. In minutes. Ramp to Used in combination with the arrival rate and duration values. Increases the arrival rate linearly to the value specified, within the specified duration. Parallelism The number of runners that will be allocated to executing the test. The arrival rate , duration and Max virtual users will be split between the specified number of runners. Max virtual users The maximum number of virtual users executing the scenario requests. This places a threshold on the number of requests that can exist simultaneously. Environment Free text describing the environment against which the test is executed. Cron expression A cron expression for scheduling the test to run periodically at a specified date/time. Run immediately Determines if the test will be executed immediately when the job is saved. Emails An email addresses to which Predator will send a message when the test execution is completed. Webhooks A URLs to which an event will be sent when the test execution is completed. The event body will include detailed information about the test, such as the number of scenarios that were executed and the number of requests that were invoked. Debug If debug is turned on, the predator runner will log every request response.","title":"Running the Test"},{"location":"myfirsttest.html#viewing-the-test-report","text":"Your curiosity is probably reaching an all-time high right now, as Predator is working hard to push your API to its limits. So how's your API performing under all that pressure? Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view a detailed report of the test that was executed. For more information about the data shown in the test reports, see Test Reports .","title":"Viewing the Test Report"},{"location":"plugins.html","text":"Plugins # Metrics and test reports are an essential part of Predator. For this, Predator can be integrated to send test metrics to either Prometheus or Influx, and to send final test results straight to your email upon test completion. SMTP Server - Email Notifier # Prometheus # InfluxDB #","title":"Plugins"},{"location":"plugins.html#plugins","text":"Metrics and test reports are an essential part of Predator. For this, Predator can be integrated to send test metrics to either Prometheus or Influx, and to send final test results straight to your email upon test completion.","title":"Plugins"},{"location":"plugins.html#smtp-server-email-notifier","text":"","title":"SMTP Server - Email Notifier"},{"location":"plugins.html#prometheus","text":"","title":"Prometheus"},{"location":"plugins.html#influxdb","text":"","title":"InfluxDB"},{"location":"processors.html","text":"Processors are used to inject custom JavaScript into the test flows. This allows the test flow to be as flexible as possible by creating application resources along the way that are needed for future requests or flow processing. Tip Processors allow you to create anything that's possible in JavaScript and integrate it seemlessly into your test flow. Use-cases # There are many use-cases for injecting custom JavaScript into your test flow. Some include: Prerequistes for a request Create resources needed for an API request, such as an authentication token, that shouldn't be calculated into the report's response times for the overall test flow. Example Random data generation Generate random data to send in your request bodies Example Response logging Log errors if exist Example Creating a processor # Creating a processor can be easily done through the Predator UI. An initial template is provided but it is not required to follow it, we recommend using appropriate function names in order to make it easier to import them into your tests. Extra documentation In order to read more documentation regarding how processors are used in artillery tests, please refer to the artillery docs . Template # module .exports = { beforeScenario , afterScenario , beforeRequest , afterResponse }; function beforeScenario ( context , ee , next ) { return next (); // MUST be called for the scenario to continue } function afterScenario ( context , ee , next ) { return next (); // MUST be called for the scenario to continue } function beforeRequest ( requestParams , context , ee , next ) { return next (); // MUST be called for the scenario to continue } function afterResponse ( requestParams , response , context , ee , next ) { return next (); // MUST be called for the scenario to continue } Description # Since Predator uses Artillery's built-in processor functionality, there are 4 instances where the processor's Javascript code can be executed. beforeScenario(context, ee, next) # These functions will be executed once before a scenario. All before scenarios functions must adhere to the (context, ee, next) parameters in their function signature. afterScenario(context, ee, next) # These functions will be executed once after a scenario. All after scenarios functions must adhere to the (context, ee, next) parameters in their function signature. beforeRequest(requestParams, context, ee, next) # These functions will be executed once before a specific request. All before requests functions must adhere to the (requestParams, context, ee, next) parameters in their function signature. afterResponse(requestParams, response, context, ee, next) # These functions will be executed once after a specific request. All after response functions must adhere to the (requestParams, response, context, ee, next) parameters in their function signature. Use case examples: # 1. Prerequistes for a request # const uuid = require ( uuid/v4 ); module .exports = { createAuthToken }; function createAuthToken ( context , ee , next ) { context . vars . token = uuid (); return next (); // MUST be called for the scenario to continue } 2. Random data generation # module .exports = { generateRandomName }; function generateRandomName ( context , ee , next ) { context . vars . name = random_name_ + Date . now (); return next (); // MUST be called for the scenario to continue } 3. Response logging # module .exports = { logErrorByStatusCode }; function logErrorByStatusCode ( requestParams , response , context , ee , next ) { if ( response . statusCode = 300 ) { console . log ( `**************** fail with status code: ${ JSON . stringify ( response . statusCode ) } ` ); console . log ( `**************** response body: ${ JSON . stringify ( response . body ) } ` ); console . log ( `**************** response headers is: ${ JSON . stringify ( response . headers ) } ` ); console . log ( `**************** request body: ${ JSON . stringify ( requestParams . body ) } ` ); } return next (); // MUST be called for the scenario to continue } Using in a test # In order to load a processor into a test, choose a processor from the list Processor in the create/edit test screen. The exported functions from the processor will dynamically load into the Before Scenario , After Scenario , Before Request , After Request dropdown menus. Example # In this example, we loaded a handleErrors processor with the following JavaScript: const uuid = require ( uuid/v4 ); module .exports = { throwErrorOnError , logResponseOnError , beforeRequest , afterResponse , afterScenario , beforeScenario }; function beforeRequest ( requestParams , context , ee , next ) { console . log ( before request ) return next (); // MUST be called for the scenario to continue } function afterResponse ( requestParams , response , context , ee , next ) { console . log ( after response ) return next (); // MUST be called for the scenario to continue } function afterScenario ( context , ee , next ) { console . log ( after scenario ) return next (); // MUST be called for the scenario to continue } function beforeScenario ( context , ee , next ) { console . log ( before scenario ) return next (); // MUST be called for the scenario to continue } function throwErrorOnError ( requestParams , response , context , ee , next ) { console . log ( **************** fail with status code: + JSON . stringify ( response . statusCode )); console . log ( **************** host is: + JSON . stringify ( response . request . uri . host )); console . log ( **************** path is: + JSON . stringify ( response . request . uri . pathname )); console . log ( **************** response headers is: + JSON . stringify ( response . headers )); throw new Error ( Stopping test ); } function logResponseOnError ( requestParams , response , context , ee , next ) { if ( response . statusCode !== 200 || esponse . statusCode !== 201 || esponse . statusCode !== 204 ){ console . log ( **************** fail with status code: + JSON . stringify ( response . statusCode )); console . log ( **************** host is: + JSON . stringify ( response . request . uri . host )); console . log ( **************** path is: + JSON . stringify ( response . request . uri . pathname )); console . log ( **************** response headers is: + JSON . stringify ( response . headers )); } return next (); // MUST be called for the scenario to continue } Scenario # beforeScenario and afterScenario functions will be executed before and after the scenario ends. Step (request) # logResponseOnError will be executed after each request.","title":"Processor Creation"},{"location":"processors.html#use-cases","text":"There are many use-cases for injecting custom JavaScript into your test flow. Some include: Prerequistes for a request Create resources needed for an API request, such as an authentication token, that shouldn't be calculated into the report's response times for the overall test flow. Example Random data generation Generate random data to send in your request bodies Example Response logging Log errors if exist Example","title":"Use-cases"},{"location":"processors.html#creating-a-processor","text":"Creating a processor can be easily done through the Predator UI. An initial template is provided but it is not required to follow it, we recommend using appropriate function names in order to make it easier to import them into your tests. Extra documentation In order to read more documentation regarding how processors are used in artillery tests, please refer to the artillery docs .","title":"Creating a processor"},{"location":"processors.html#template","text":"module .exports = { beforeScenario , afterScenario , beforeRequest , afterResponse }; function beforeScenario ( context , ee , next ) { return next (); // MUST be called for the scenario to continue } function afterScenario ( context , ee , next ) { return next (); // MUST be called for the scenario to continue } function beforeRequest ( requestParams , context , ee , next ) { return next (); // MUST be called for the scenario to continue } function afterResponse ( requestParams , response , context , ee , next ) { return next (); // MUST be called for the scenario to continue }","title":"Template"},{"location":"processors.html#description","text":"Since Predator uses Artillery's built-in processor functionality, there are 4 instances where the processor's Javascript code can be executed.","title":"Description"},{"location":"processors.html#beforescenariocontext-ee-next","text":"These functions will be executed once before a scenario. All before scenarios functions must adhere to the (context, ee, next) parameters in their function signature.","title":"beforeScenario(context, ee, next)"},{"location":"processors.html#afterscenariocontext-ee-next","text":"These functions will be executed once after a scenario. All after scenarios functions must adhere to the (context, ee, next) parameters in their function signature.","title":"afterScenario(context, ee, next)"},{"location":"processors.html#beforerequestrequestparams-context-ee-next","text":"These functions will be executed once before a specific request. All before requests functions must adhere to the (requestParams, context, ee, next) parameters in their function signature.","title":"beforeRequest(requestParams, context, ee, next)"},{"location":"processors.html#afterresponserequestparams-response-context-ee-next","text":"These functions will be executed once after a specific request. All after response functions must adhere to the (requestParams, response, context, ee, next) parameters in their function signature.","title":"afterResponse(requestParams, response, context, ee, next)"},{"location":"processors.html#use-case-examples","text":"","title":"Use case examples:"},{"location":"processors.html#1-prerequistes-for-a-request","text":"const uuid = require ( uuid/v4 ); module .exports = { createAuthToken }; function createAuthToken ( context , ee , next ) { context . vars . token = uuid (); return next (); // MUST be called for the scenario to continue }","title":"1. Prerequistes for a request"},{"location":"processors.html#2-random-data-generation","text":"module .exports = { generateRandomName }; function generateRandomName ( context , ee , next ) { context . vars . name = random_name_ + Date . now (); return next (); // MUST be called for the scenario to continue }","title":"2. Random data generation"},{"location":"processors.html#3-response-logging","text":"module .exports = { logErrorByStatusCode }; function logErrorByStatusCode ( requestParams , response , context , ee , next ) { if ( response . statusCode = 300 ) { console . log ( `**************** fail with status code: ${ JSON . stringify ( response . statusCode ) } ` ); console . log ( `**************** response body: ${ JSON . stringify ( response . body ) } ` ); console . log ( `**************** response headers is: ${ JSON . stringify ( response . headers ) } ` ); console . log ( `**************** request body: ${ JSON . stringify ( requestParams . body ) } ` ); } return next (); // MUST be called for the scenario to continue }","title":"3. Response logging"},{"location":"processors.html#using-in-a-test","text":"In order to load a processor into a test, choose a processor from the list Processor in the create/edit test screen. The exported functions from the processor will dynamically load into the Before Scenario , After Scenario , Before Request , After Request dropdown menus.","title":"Using in a test"},{"location":"processors.html#example","text":"In this example, we loaded a handleErrors processor with the following JavaScript: const uuid = require ( uuid/v4 ); module .exports = { throwErrorOnError , logResponseOnError , beforeRequest , afterResponse , afterScenario , beforeScenario }; function beforeRequest ( requestParams , context , ee , next ) { console . log ( before request ) return next (); // MUST be called for the scenario to continue } function afterResponse ( requestParams , response , context , ee , next ) { console . log ( after response ) return next (); // MUST be called for the scenario to continue } function afterScenario ( context , ee , next ) { console . log ( after scenario ) return next (); // MUST be called for the scenario to continue } function beforeScenario ( context , ee , next ) { console . log ( before scenario ) return next (); // MUST be called for the scenario to continue } function throwErrorOnError ( requestParams , response , context , ee , next ) { console . log ( **************** fail with status code: + JSON . stringify ( response . statusCode )); console . log ( **************** host is: + JSON . stringify ( response . request . uri . host )); console . log ( **************** path is: + JSON . stringify ( response . request . uri . pathname )); console . log ( **************** response headers is: + JSON . stringify ( response . headers )); throw new Error ( Stopping test ); } function logResponseOnError ( requestParams , response , context , ee , next ) { if ( response . statusCode !== 200 || esponse . statusCode !== 201 || esponse . statusCode !== 204 ){ console . log ( **************** fail with status code: + JSON . stringify ( response . statusCode )); console . log ( **************** host is: + JSON . stringify ( response . request . uri . host )); console . log ( **************** path is: + JSON . stringify ( response . request . uri . pathname )); console . log ( **************** response headers is: + JSON . stringify ( response . headers )); } return next (); // MUST be called for the scenario to continue }","title":"Example"},{"location":"processors.html#scenario","text":"beforeScenario and afterScenario functions will be executed before and after the scenario ends.","title":"Scenario"},{"location":"processors.html#step-request","text":"logResponseOnError will be executed after each request.","title":"Step (request)"},{"location":"schedulesandreports.html","text":"Scheduled Tasks # Predator creates scheduled tasks for all tests that you scheduled to run at predefined intervals. To see the scheduled tasks that are registered in the system, choose Tasks Scheduled Tasks Test Reports # Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view detailed reports of the tests that were executed.","title":"Schedules and Reports"},{"location":"schedulesandreports.html#scheduled-tasks","text":"Predator creates scheduled tasks for all tests that you scheduled to run at predefined intervals. To see the scheduled tasks that are registered in the system, choose Tasks Scheduled Tasks","title":"Scheduled Tasks"},{"location":"schedulesandreports.html#test-reports","text":"Predator's test reports will give you the in-depth performance metrics you're looking for. From the Predator UI choose Reports Last Reports to view detailed reports of the tests that were executed.","title":"Test Reports"},{"location":"tests.html","text":"What you should already know We assume you have already familiarized yourself with the basic concepts used in Predator and that you successfully created your first test. If not, we strongly recommend you first complete the steps in the My First Test topic before proceeding with the instructions below. Data Reuse with Variables # The Captures field of the HTTP request properties allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. Use JSONPath syntax to extract the data of your choice. In the following example we extract the id field and store it in a petId variable: You can then use petId in a request by placing it between double brackets, like so: {{petId}} Here's an example of using {{petId}} in the request path: Pick a random element from an array # When capturing responses data, you may want to randomly select an item from your dataset to continue with your scenario. This is achieved by using the * operator, which selects all the matching items. The engine will then randomly pick a single item for you and store it in the defined variable. Usage example : on [ { id : 45697038-37ae-4149-9f5e-ed7a4a20e014 , data : 132456789 }, { id : f02392e8-1b11-401f-a902-8a9a1ae5c47a , data : abcdefghi } ] will randomly store 45697038-...-ed7a4a20e014 or f02392e8-...-8a9a1ae5c47a in {{dataId}} for further usage in the scenario. Built-in Functions # Predator supports some generic functions out of the box and they can be used to generate random data. $randomNumber(min, max) will generate a random number within the provided range. $randomString(length) will generate a random string with the specified length. $uuid() will generate v4 UUID. $dateNow() will generate a number of ms since epoch. Currently these functions all return values as a string . An issue is open regarding this: #178 Usage example: { id : {{ $uuid() }} , name : {{ $randomString(6) }} , age : {{ $randomNumber(0,15) }} , created : {{ $dateNow() }} } Request Reuse with DSL Definitions # This is the moment where Predator shows its teeth and unleashes its true power. Writing a performance test that checks specific parts of your API end-to-end can be a huge hassle, but now it is effortless. By creating DSL definitions using Predator's Domain Specific Language (DSL), request templates are generated which you can then reuse in the same test and in other tests under the same DSL type, reducing replication. Let's dive right in and get going with our first DSL definition. Predator API This functionality is only available through the Predator API . Creating a DSL Definition # Before you can use a DSL definition, you must create it first. You do so by invoking the Create DSL Definition request. Here's an example request body for creating a DSL definition of a GET request. Notice how we use the {{petId}} in the url endpoint (we will create this variable in the example of a POST request DSL definition): { name : get-pet , request : { get : { url : http://127.0.0.1:3000/pets/{{petId}} } } } The request body for creating a DSL definition of a POST request is a bit more elaborate, since it requires that you pass in the entire body that makes up the POST request. The following example shows how to do this. Notice how we add a capture array, in which we define the petId variable for storing the pet ID. We can then reuse it in another request (like in the example of a GET request above). { name : create-pet , request : { post : { url : http://127.0.0.1:3000/pets , json : { name : mickey , species : dog }, capture : [ { json : $.id , as : petId } ] } } } Creating a Test that Uses the DSL # Tests that use a DSL definition can only be created using the Create Test API request. The Create Test API request body must include all components that make up a test, including pre-scenario requests and scenarios. However, instead of defining the entire HTTP request in each scenario step (as you would through the Predator UI), you can now reference the HTTP request through its DSL definition. You do so, using the action property (in the steps array). Here's an example: { name : Pet store , description : DSL , type : dsl , // Make sure the type is set to DSL before : { steps : [ { action : petstore.create-pet } ] }, scenarios : [ { scenario_name : Only get pet , steps : [ { action : petstore.get-pet } ] } ] } There are two additional items to note: The type must always be set to dsl . The action value uses the following syntax: {dsl_group}.{dsl_name} , in which the dsl_group is the name used in the path of the Create DSL Definition API request. If you login to the Predator UI after creating the test, you will notice that the test has been added with a type of dsl . You can now run the test as you would any other. Creating a Test with Custom Logic in Javascript (Released in version 1.2.0) # Tests can use custom Javascript functions. To use custom Javascript functions in your tests please refer to the processors documentation . Debugging Test Requests/Responses # It is possible to view all of the requests and responses that the predator-runner sends and receives while running load on the API. This is very useful when your test does not behave the way you expected it to and need the request/response to further investigate. The predator-runner will log this data to the log files which can be downloaded. For the test to run in debug mode: 1. UI # Turn on the Debug flag when running a test 2. API # Add \"debug\": \"*\" to the body of the POST /jobs request","title":"Advanced Test Setup"},{"location":"tests.html#data-reuse-with-variables","text":"The Captures field of the HTTP request properties allows you to extract (capture) data from the request response and store it in a variable for reuse in another request. Use JSONPath syntax to extract the data of your choice. In the following example we extract the id field and store it in a petId variable: You can then use petId in a request by placing it between double brackets, like so: {{petId}} Here's an example of using {{petId}} in the request path:","title":"Data Reuse with Variables"},{"location":"tests.html#pick-a-random-element-from-an-array","text":"When capturing responses data, you may want to randomly select an item from your dataset to continue with your scenario. This is achieved by using the * operator, which selects all the matching items. The engine will then randomly pick a single item for you and store it in the defined variable. Usage example : on [ { id : 45697038-37ae-4149-9f5e-ed7a4a20e014 , data : 132456789 }, { id : f02392e8-1b11-401f-a902-8a9a1ae5c47a , data : abcdefghi } ] will randomly store 45697038-...-ed7a4a20e014 or f02392e8-...-8a9a1ae5c47a in {{dataId}} for further usage in the scenario.","title":"Pick a random element from an array"},{"location":"tests.html#built-in-functions","text":"Predator supports some generic functions out of the box and they can be used to generate random data. $randomNumber(min, max) will generate a random number within the provided range. $randomString(length) will generate a random string with the specified length. $uuid() will generate v4 UUID. $dateNow() will generate a number of ms since epoch. Currently these functions all return values as a string . An issue is open regarding this: #178 Usage example: { id : {{ $uuid() }} , name : {{ $randomString(6) }} , age : {{ $randomNumber(0,15) }} , created : {{ $dateNow() }} }","title":"Built-in Functions"},{"location":"tests.html#request-reuse-with-dsl-definitions","text":"This is the moment where Predator shows its teeth and unleashes its true power. Writing a performance test that checks specific parts of your API end-to-end can be a huge hassle, but now it is effortless. By creating DSL definitions using Predator's Domain Specific Language (DSL), request templates are generated which you can then reuse in the same test and in other tests under the same DSL type, reducing replication. Let's dive right in and get going with our first DSL definition. Predator API This functionality is only available through the Predator API .","title":"Request Reuse with DSL Definitions"},{"location":"tests.html#creating-a-dsl-definition","text":"Before you can use a DSL definition, you must create it first. You do so by invoking the Create DSL Definition request. Here's an example request body for creating a DSL definition of a GET request. Notice how we use the {{petId}} in the url endpoint (we will create this variable in the example of a POST request DSL definition): { name : get-pet , request : { get : { url : http://127.0.0.1:3000/pets/{{petId}} } } } The request body for creating a DSL definition of a POST request is a bit more elaborate, since it requires that you pass in the entire body that makes up the POST request. The following example shows how to do this. Notice how we add a capture array, in which we define the petId variable for storing the pet ID. We can then reuse it in another request (like in the example of a GET request above). { name : create-pet , request : { post : { url : http://127.0.0.1:3000/pets , json : { name : mickey , species : dog }, capture : [ { json : $.id , as : petId } ] } } }","title":"Creating a DSL Definition"},{"location":"tests.html#creating-a-test-that-uses-the-dsl","text":"Tests that use a DSL definition can only be created using the Create Test API request. The Create Test API request body must include all components that make up a test, including pre-scenario requests and scenarios. However, instead of defining the entire HTTP request in each scenario step (as you would through the Predator UI), you can now reference the HTTP request through its DSL definition. You do so, using the action property (in the steps array). Here's an example: { name : Pet store , description : DSL , type : dsl , // Make sure the type is set to DSL before : { steps : [ { action : petstore.create-pet } ] }, scenarios : [ { scenario_name : Only get pet , steps : [ { action : petstore.get-pet } ] } ] } There are two additional items to note: The type must always be set to dsl . The action value uses the following syntax: {dsl_group}.{dsl_name} , in which the dsl_group is the name used in the path of the Create DSL Definition API request. If you login to the Predator UI after creating the test, you will notice that the test has been added with a type of dsl . You can now run the test as you would any other.","title":"Creating a Test that Uses the DSL"},{"location":"tests.html#creating-a-test-with-custom-logic-in-javascript-released-in-version-120","text":"Tests can use custom Javascript functions. To use custom Javascript functions in your tests please refer to the processors documentation .","title":"Creating a Test with Custom Logic in Javascript (Released in version 1.2.0)"},{"location":"tests.html#debugging-test-requestsresponses","text":"It is possible to view all of the requests and responses that the predator-runner sends and receives while running load on the API. This is very useful when your test does not behave the way you expected it to and need the request/response to further investigate. The predator-runner will log this data to the log files which can be downloaded. For the test to run in debug mode:","title":"Debugging Test Requests/Responses"},{"location":"tests.html#1-ui","text":"Turn on the Debug flag when running a test","title":"1. UI"},{"location":"tests.html#2-api","text":"Add \"debug\": \"*\" to the body of the POST /jobs request","title":"2. API"}]}